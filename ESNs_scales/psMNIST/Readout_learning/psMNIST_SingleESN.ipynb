{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.client import timeline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-78a3e82a0fbd>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\lucam\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\lucam\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\lucam\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\lucam\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\lucam\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\lucam\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_tr=mnist.train.images\n",
    "Y_tr=mnist.train.labels\n",
    "X_te=mnist.test.images\n",
    "Y_te=mnist.test.labels\n",
    "X_val=mnist.validation.images\n",
    "Y_val=mnist.validation.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19571953da0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.reshape(X_tr[1,:],[28,28]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply permutation for the psMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "P=np.shape(X_tr)[1]\n",
    "permutation=np.random.permutation(P)\n",
    "\n",
    "X_tr=X_tr[:,permutation]\n",
    "X_te=X_te[:,permutation]\n",
    "X_val=X_val[:,permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class to compute the ESN response\n",
    "\n",
    "The input should be:\n",
    "<ul>\n",
    "    <li> alpha1: leakage term of the reservoir </li>\n",
    "    <li> pho1: $\\rho$ of the reservoir </li>\n",
    "    <li> diluition1: probability of a zero in the connectivity matrix </li> \n",
    "    <li> N1: number of nodes of the ESN </li>\n",
    "    <li> W1_in: input connectivity matrix </li>\n",
    "    <li> T_conc: the multiples of T_conc define the values for the readout </li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Echo:\n",
    "    \n",
    "    def __init__(self,alpha1, pho1, diluition1, N1, W1_in, T_conc):\n",
    "                \n",
    "            ## Reservoir 1\n",
    "        \n",
    "            self.N1=N1\n",
    "            \n",
    "            ## alpha 1\n",
    "            self.alpha1=tf.constant(alpha1,dtype=tf.float32)\n",
    "            \n",
    "            ## W1 def\n",
    "            W1_np=np.random.uniform(-1,1,[N1,N1])\n",
    "            D=np.random.uniform(0,1,(N1,N1))>np.ones((N1,N1))*diluition1\n",
    "            W1_np=W1_np*D.astype(int)\n",
    "            \n",
    "            eig1=np.linalg.eigvals(W1_np)\n",
    "\n",
    "            W1_np=pho1*W1_np/(np.max(np.absolute(eig1)))\n",
    "            self.W1=tf.Variable(W1_np,trainable=False,dtype=tf.float32)\n",
    "            \n",
    "            self.eig1=eig1\n",
    "            \n",
    "            ## Input W\n",
    "            self.W1_in=tf.Variable(W1_in,trainable=False,dtype=tf.float32)\n",
    "            \n",
    "            ## The multiples of T_conc are the values of t at which the ESN representations are concatenated\n",
    "            self.T_conc=T_conc\n",
    "            \n",
    "    \n",
    "    # Computation of the states of the reservoir\n",
    "    def train_graph(self,T,init_state1,inputs,return_all=False):\n",
    "\n",
    "        \n",
    "        state1=init_state1\n",
    "        states_train1=[]\n",
    "        \n",
    "        \n",
    "        for t in range(T):\n",
    "            \n",
    "            prev_state1=tf.identity(state1)\n",
    "            \n",
    "            state1 = (1-self.alpha1)*prev_state1+self.alpha1*tf.tanh( tf.matmul(prev_state1,self.W1)\\\n",
    "                                                                     +tf.tile(tf.expand_dims(inputs[:,t],1),[1,self.N1])*self.W1_in )\n",
    "                                                    \n",
    "            \n",
    "            if return_all:\n",
    "                \n",
    "                states_train1.append(state1)\n",
    "                \n",
    "                \n",
    "            elif (t+1)%self.T_conc==0:\n",
    "                \n",
    "                states_train1.append(state1)\n",
    "                \n",
    "                \n",
    "                \n",
    "        states1=tf.concat([tf.expand_dims(s,2) for s in states_train1],2)\n",
    "        \n",
    "        \n",
    "        return  states1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the input connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_input(N1):\n",
    "\n",
    "    W1_In=np.ones(N1)\n",
    "    W1_In[np.random.uniform(0,1,N1)<0.5]=-1\n",
    "    \n",
    "    \n",
    "    return W1_In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESN representation over the dataset\n",
    "\n",
    "The data are splitted because of the limited memory in my gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Echo_representations(alpha1, pho1, diluition1, N1, W1_in, X_tr, X_val, X_test, T_size):\n",
    "\n",
    "    T=np.shape(X_tr)[1]\n",
    "    N_class=np.shape(Y_tr)[1]\n",
    "    \n",
    "    T_conc=np.int(T/T_size)\n",
    "    \n",
    "    rnn=Echo(alpha1, pho1, diluition1, N1, W1_in, T_conc)\n",
    "\n",
    "    init_state1=tf.placeholder(tf.float32,[None,N1])\n",
    "    s=tf.placeholder(tf.float32,[None,T])\n",
    "\n",
    "    states=rnn.train_graph(T,init_state1,s,return_all=False)\n",
    "\n",
    "    init=tf.global_variables_initializer()\n",
    "    \n",
    "    train_divide=100\n",
    "    N_train_d=int(np.floor(np.shape(Y_tr)[0]/train_divide))\n",
    "\n",
    "    test_divide=50\n",
    "    N_test_d=int(np.floor(np.shape(Y_te)[0]/test_divide))\n",
    "    \n",
    "    val_divide=50\n",
    "    N_val_d=int(np.floor(np.shape(Y_val)[0]/val_divide))\n",
    "\n",
    "    States=np.zeros([N_train_d*train_divide,N,T_size])\n",
    "    States_test=np.zeros([N_test_d*test_divide,N,T_size])\n",
    "    States_val=np.zeros([N_val_d*val_divide,N,T_size])\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(init)\n",
    "        \n",
    "        print('Training Data')\n",
    "\n",
    "        for l in range(train_divide):\n",
    "\n",
    "            images=np.copy(X_tr[l*N_train_d:(l+1)*N_train_d,:])\n",
    "\n",
    "            states_=sess.run(states,feed_dict={init_state1:np.zeros([N_train_d,N1]),s:images})\n",
    "\n",
    "            States[l*N_train_d:(l+1)*N_train_d,:,:]=states_\n",
    "\n",
    "            \n",
    "        print('Testing Data')\n",
    "\n",
    "        for l in range(test_divide):\n",
    "\n",
    "            images=np.copy(X_te[l*N_test_d:(l+1)*N_test_d,:])\n",
    "\n",
    "            states_=sess.run(states,feed_dict={init_state1:np.zeros([N_test_d,N1]),s:images})\n",
    "\n",
    "            States_test[l*N_test_d:(l+1)*N_test_d,:,:]=states_    \n",
    "            \n",
    "       \n",
    "        print('Validating Data')\n",
    "\n",
    "        for l in range(val_divide):\n",
    "\n",
    "            images=np.copy(X_val[l*N_val_d:(l+1)*N_val_d,:])\n",
    "\n",
    "            states_=sess.run(states,feed_dict={init_state1:np.zeros([N_val_d,N1]),s:images})\n",
    "\n",
    "            States_val[l*N_val_d:(l+1)*N_val_d,:,:]=states_\n",
    "        \n",
    "    \n",
    "    return States, States_test, States_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow graph for the training\n",
    "It loops over the parameters considered in the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation  9 , alpha1= 0.006737946999085467\n",
      "Processing Resrvoir Representations: \n",
      "Training Data\n",
      "Testing Data\n",
      "Validating Data\n",
      "Start Training: \n",
      "Iteration:  10000 Reservoir  0 Probability:  0.7350000143051147 Error:  0.15922730416059494\n",
      "Iteration:  30000 Reservoir  0 Probability:  0.7173999845981598 Error:  0.16655051708221436\n",
      "Iteration:  50000 Reservoir  0 Probability:  0.7409999966621399 Error:  0.15850575268268585\n",
      "Iteration:  70000 Reservoir  0 Probability:  0.6933999955654144 Error:  0.185210719704628\n",
      "Iteration:  90000 Reservoir  0 Probability:  0.7236000001430511 Error:  0.16543208807706833\n",
      "Iteration:  91000 Reservoir  0 Probability:  0.7502000033855438 Error:  0.14900782704353333\n",
      "Iteration:  92000 Reservoir  0 Probability:  0.7525999844074249 Error:  0.14964110404253006\n",
      "Iteration:  93000 Reservoir  0 Probability:  0.7452000081539154 Error:  0.15165464580059052\n",
      "Iteration:  94000 Reservoir  0 Probability:  0.7477999925613403 Error:  0.16017241030931473\n",
      "Iteration:  95000 Reservoir  0 Probability:  0.7653999924659729 Error:  0.14649251103401184\n",
      "Iteration:  96000 Reservoir  0 Probability:  0.756600022315979 Error:  0.14699259400367737\n",
      "Iteration:  97000 Reservoir  0 Probability:  0.7506000101566315 Error:  0.1490374058485031\n",
      "Iteration:  98000 Reservoir  0 Probability:  0.7405999898910522 Error:  0.16254670917987823\n",
      "Iteration:  99000 Reservoir  0 Probability:  0.7355999946594238 Error:  0.15503089874982834\n",
      "Simulation  19 , alpha1= 0.01174362845702136\n",
      "Processing Resrvoir Representations: \n",
      "Training Data\n",
      "Testing Data\n",
      "Validating Data\n",
      "Start Training: \n",
      "Iteration:  10000 Reservoir  1 Probability:  0.6811999976634979 Error:  0.18025030195713043\n",
      "Iteration:  30000 Reservoir  1 Probability:  0.7326000034809113 Error:  0.1604546681046486\n",
      "Iteration:  50000 Reservoir  1 Probability:  0.7556000053882599 Error:  0.14512646943330765\n",
      "Iteration:  70000 Reservoir  1 Probability:  0.7504000067710876 Error:  0.15113504230976105\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b97d1dc0d55c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     96\u001b[0m                     \u001b[0mrand_ind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                     \u001b[0mimages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mStates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrand_ind\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                     \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrand_ind\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy import io\n",
    "\n",
    "## Simulation \n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "N_size=np.shape(X_tr)[0]\n",
    "\n",
    "# Training parameters\n",
    "N_train=100000          # Number of training iteration\n",
    "alpha_size=0.0005       # Learning rate\n",
    "batch_size=20\n",
    "T_size=28               # Number of hidden states used for the readout (they are equally spaced across the input sequence)\n",
    "\n",
    "N_class=np.shape(Y_tr)[1]\n",
    "\n",
    "#  Defines the values at which the performance are computed \n",
    "N_check=15\n",
    "T_checks=np.concatenate([np.linspace(10000,90000,5),np.linspace(91000,100000,N_check-5)],0) \n",
    "\n",
    "# Number of times the test, validation and training dataset are divided\n",
    "test_divide=2\n",
    "val_divide=2\n",
    "train_divide=10\n",
    "\n",
    "N_test_d=int(np.floor(np.shape(Y_te)[0]/test_divide))\n",
    "N_val_d=int(np.floor(np.shape(Y_val)[0]/val_divide))\n",
    "N_train_d=int(np.floor(np.shape(Y_tr)[0]/train_divide))\n",
    "\n",
    "N_grid=10\n",
    "\n",
    "p_ra_test=np.zeros([N_grid,N_grid,N_check])\n",
    "p_ra_train=np.zeros([N_grid,N_grid,N_check])\n",
    "p_ra_val=np.zeros([N_grid,N_grid,N_check])\n",
    "\n",
    "## Network parameters\n",
    "N1=1200                 # Network size\n",
    "N1_av=5                 # Average number of connections for a node in the ESN\n",
    "\n",
    "diluition1=1-N1_av/N1   # Probability of a zero in the connectivity matrix\n",
    "\n",
    "# Values of alpha used, the program computes the performance for each of them\n",
    "alpha1=np.exp(np.linspace(-5,0,N_grid)) \n",
    "\n",
    "pho1=0.985              # Value of \\rho\n",
    "gamma1=1                # Multiplicative factor of the input connectivity\n",
    "\n",
    "    \n",
    "## Training    \n",
    "        \n",
    "for i in range(0,N_grid):\n",
    "\n",
    "    for j in range(N_grid-1,N_grid): # This loop exists if one wants to do a grid search over another parameter\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        index_help=0\n",
    "\n",
    "        W1_in=W_input(N1)\n",
    "        W1_in=gamma1*W1_in.T\n",
    "\n",
    "        print('Simulation ', i*N_grid+j, ', alpha1=',alpha1[i])\n",
    "        print('Processing Resrvoir Representations: ')\n",
    "        \n",
    "        # Computation of the ESN representation, saved in the arrays States (train), States_test (Test), States_val (validation)\n",
    "        States, States_test, States_val=Echo_representations(alpha1[i], pho1, diluition1, N1, W1_in,\n",
    "                                 X_tr, X_val, X_te, T_size)\n",
    "\n",
    "        print('Start Training: ')\n",
    "        \n",
    "        # Definition of the graph fpr the training\n",
    "        # The placeholder s receives the representations of the ESN used for the readout (for the specific batch_size sequences considered) \n",
    "        s=tf.placeholder(tf.float32,[None,N,T_size])\n",
    "        V=tf.reshape(s,[-1,T_size*N])\n",
    "\n",
    "        y_true=tf.placeholder(tf.float32,[None,N_class])\n",
    "\n",
    "        W_out=tf.Variable(np.random.uniform(-1,1,[N*T_size,N_class])/(N),dtype=tf.float32)\n",
    "\n",
    "        y=tf.matmul(V,W_out)\n",
    "\n",
    "        error=tf.losses.sigmoid_cross_entropy(y_true,y)\n",
    "\n",
    "        train=tf.train.AdamOptimizer(learning_rate=alpha_size).minimize(error,var_list=[W_out])\n",
    "\n",
    "        init=tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(init)\n",
    "\n",
    "            for n in range(N_train):\n",
    "\n",
    "                if n>0:\n",
    "\n",
    "                    rand_ind=np.random.randint(0,N_size,(batch_size,))\n",
    "\n",
    "                    images=States[rand_ind,:,:]\n",
    "\n",
    "                    labels=Y_tr[rand_ind,:]\n",
    "\n",
    "                    _=sess.run(train,feed_dict={y_true:labels,s:images})\n",
    "\n",
    "                # If the following condition is true, the program will compute the performance over the test and validation data\n",
    "                if n==T_checks[index_help]:\n",
    "\n",
    "                    index_help=index_help+1\n",
    "                    \n",
    "                    matches=tf.equal(tf.argmax(y_true,1),tf.argmax(y,1))\n",
    "                    p=tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "                    \n",
    "                    \n",
    "                    # Performance on the test data, computed as a mean over the different splits of the dataset through the variable p_ra_ and error_ \n",
    "                    p_ra_=0\n",
    "                    error_=0\n",
    "\n",
    "                    for l in range(test_divide):\n",
    "\n",
    "                        images=States_test[l*N_test_d:(l+1)*N_test_d,:,:]\n",
    "                        labels=Y_te[l*N_test_d:(l+1)*N_test_d,:]\n",
    "\n",
    "                        p_ra1_,error1_=sess.run([p,error],feed_dict={y_true:labels,s:images})\n",
    "\n",
    "                        p_ra_=p_ra_+p_ra1_/test_divide\n",
    "                        error_=error_+error1_/test_divide\n",
    "\n",
    "\n",
    "                    p_ra_test[i,j,index_help]=p_ra_\n",
    "\n",
    "                    #print('Iteration: ',n,'Reservoir ',i,'Probability: ',p_ra_,'Error: ',error_)\n",
    "\n",
    "                    # Performance on the validation data, computed as a mean over the different splits of the dataset through the variable p_ra_ and error_ \n",
    "                    p_ra_v=0\n",
    "                    error_v=0\n",
    "\n",
    "                    for l in range(val_divide):\n",
    "\n",
    "                        images=States_val[l*N_val_d:(l+1)*N_val_d,:,:]\n",
    "                        labels=Y_val[l*N_val_d:(l+1)*N_val_d,:]\n",
    "\n",
    "                        p_ra3_,error3_=sess.run([p,error],feed_dict={y_true:labels,s:images})\n",
    "\n",
    "                        p_ra_v=p_ra_v+p_ra3_/val_divide\n",
    "                        error_v=error_v+error3_/val_divide\n",
    "\n",
    "                    p_ra_val[i,j,index_help]=p_ra_v\n",
    "\n",
    "                    print('Iteration: ',n,'Reservoir ',i,'Probability: ',p_ra_v,'Error: ',error_v)\n",
    "\n",
    "            # Saving the data\n",
    "            #io.savemat(\"AccSingleEcho1_test_\"+repr(i)+\"_\"+repr(j)+\".mat\",{\"array\": p_ra_test[i,j,:]})\n",
    "            #io.savemat(\"AccSingleEcho1_val_\"+repr(i)+\"_\"+repr(j)+\".mat\",{\"array\": p_ra_val[i,j,:]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.savemat(\"Alphas.mat\",{\"array\": alpha1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13.1",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
