{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.client import timeline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-78a3e82a0fbd>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\lucam\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\lucam\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\lucam\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\lucam\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\lucam\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_tr=mnist.train.images\n",
    "Y_tr=mnist.train.labels\n",
    "X_te=mnist.test.images\n",
    "Y_te=mnist.test.labels\n",
    "X_val=mnist.validation.images\n",
    "Y_val=mnist.validation.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_val)\n",
    "np.shape(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x153c4b30da0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.reshape(X_tr[1,:],[28,28]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply permutation for the psMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "P=np.shape(X_tr)[1]\n",
    "permutation=np.random.permutation(P)\n",
    "\n",
    "X_tr=X_tr[:,permutation]\n",
    "X_te=X_te[:,permutation]\n",
    "X_val=X_val[:,permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#io.savemat(\"X_tr_p.mat\",{\"array\": X_tr})\n",
    "#io.savemat(\"Y_tr_p.mat\",{\"array\": Y_tr})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class to compute the ESN response\n",
    "\n",
    "The input should be:\n",
    "<ul>\n",
    "    <li> alpha1, alpha2: leakage term of the reservoir 1 and 2 </li>\n",
    "    <li> pho1, pho2: $\\rho$ of the reservoir 1 and 2 </li>\n",
    "    <li> diluition1, diluition2: probability of a zero in the connectivity matrix of ESN 1 and 2 </li> \n",
    "    <li> N1, N2: number of nodes of ESN 1 and 2 </li>\n",
    "    <li> W1_in, W2_in: input connectivity matrix of ESN 1 and 2 </li>\n",
    "    <li> gamma12: multiplicative factor of the connectivity matrix between the two ESN\n",
    "    <li> T_conc: the multiples of T_conc define the values for the readout </li>\n",
    "    \n",
    "</ul>\n",
    "\n",
    "Note: ESN 1 is the 'deeper' one and receives input from ESN 2 in the case of hierarchical structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Echo:\n",
    "    \n",
    "    def __init__(self,alpha1, pho1, diluition1, N1, W1_in,\n",
    "                         alpha2, pho2, diluition2, N2, W2_in, gamma12, T_conc):\n",
    "                \n",
    "            ## Reservoir 1\n",
    "        \n",
    "            self.N1=N1\n",
    "            \n",
    "            ## alpha 1\n",
    "            self.alpha1=tf.constant(alpha1,dtype=tf.float32)\n",
    "            \n",
    "            ## W1 def\n",
    "            W1_np=np.random.uniform(-1,1,[N1,N1])\n",
    "            D=np.random.uniform(0,1,(N1,N1))>np.ones((N1,N1))*diluition1\n",
    "            W1_np=W1_np*D.astype(int)\n",
    "            \n",
    "            eig1=np.linalg.eigvals(W1_np)\n",
    "\n",
    "            W1_np=pho1*W1_np/(np.max(np.absolute(eig1)))\n",
    "            self.W1=tf.Variable(W1_np,trainable=False,dtype=tf.float32)\n",
    "            \n",
    "            \n",
    "            self.eig1=eig1\n",
    "            \n",
    "            ## Input W\n",
    "            self.W1_in=tf.Variable(W1_in,trainable=False,dtype=tf.float32)\n",
    "            \n",
    "            \n",
    "            ## Reservoir 2\n",
    "            \n",
    "            self.N2=N2\n",
    "            \n",
    "            ## alpha 2\n",
    "            self.alpha2=tf.constant(alpha2,dtype=tf.float32)\n",
    "            \n",
    "            ## W2 def\n",
    "            W2_np=np.random.uniform(-1,1,[N2,N2])\n",
    "            D=np.random.uniform(0,1,(N2,N2))>np.ones((N2,N2))*diluition2\n",
    "            W2_np=W2_np*D.astype(int)\n",
    "\n",
    "            eig2=np.linalg.eigvals(W2_np)\n",
    "            \n",
    "            W2_np=pho2*W2_np/(np.max(np.absolute(eig2)))\n",
    "            self.W2=tf.Variable(W2_np,trainable=False,dtype=tf.float32)    \n",
    "                     \n",
    "            ## Input W    \n",
    "            self.W2_in=tf.Variable(W2_in,trainable=False,dtype=tf.float32)\n",
    "            \n",
    "            ## Reservoir 2 to Reservoir 1\n",
    "            \n",
    "            W12=gamma12*np.random.randn(N2,N1)\n",
    "            \n",
    "            self.W12=tf.Variable(W12,trainable=False,dtype=tf.float32)\n",
    "            \n",
    "            ## The multiples of T_conc are the values of t at which the ESN representations are concatenated\n",
    "            self.T_conc=T_conc\n",
    "            \n",
    "    def train_graph(self,T,init_state1,init_state2,inputs,return_all=False):\n",
    "\n",
    "        \n",
    "        state1=init_state1\n",
    "        states_train1=[]\n",
    "        \n",
    "        state2=init_state2\n",
    "        states_train2=[]\n",
    "\n",
    "        \n",
    "        \n",
    "        for t in range(T):\n",
    "            \n",
    "            prev_state1=tf.identity(state1)\n",
    "            prev_state2=tf.identity(state2)\n",
    "            \n",
    "            state1 = (1-self.alpha1)*prev_state1+self.alpha1*tf.tanh( tf.matmul(prev_state1,self.W1)\\\n",
    "                                                    \n",
    "                                                                     +tf.matmul(prev_state2,self.W12)\\\n",
    "                                                                     \n",
    "                                                                     +tf.tile(tf.expand_dims(inputs[:,t],1),[1,self.N1])*self.W1_in )\n",
    "                                                    \n",
    "           \n",
    "            state2 = (1-self.alpha2)*prev_state2+self.alpha2*tf.tanh( tf.matmul(prev_state2,self.W2)\\\n",
    "                                                                     \n",
    "                                                                     +tf.tile(tf.expand_dims(inputs[:,t],1),[1,self.N2])*self.W2_in )\n",
    "            \n",
    "            \n",
    "            if return_all:\n",
    "                \n",
    "                states_train1.append(state1)\n",
    "                \n",
    "                states_train2.append(state2)\n",
    "                \n",
    "            elif (t+1)%self.T_conc==0:\n",
    "                \n",
    "                states_train1.append(state1)\n",
    "                \n",
    "                states_train2.append(state2)\n",
    "                \n",
    "                \n",
    "        states1=tf.concat([tf.expand_dims(s,2) for s in states_train1],2)\n",
    "        \n",
    "        states2=tf.concat([tf.expand_dims(s,2) for s in states_train2],2)\n",
    "        \n",
    "        states=tf.concat([states1,states2],axis=1)\n",
    "        \n",
    "        return states, states1, states2\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the input connectivity matrices\n",
    "In the case of hierarchical ESN, W1_In will be set to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_input(N1,N2):\n",
    "\n",
    "    W1_In=np.ones(N1)\n",
    "    W1_In[np.random.uniform(0,1,N1)<0.5]=-1\n",
    "    \n",
    "    W2_In=np.ones(N2)\n",
    "    W2_In[np.random.uniform(0,1,N2)<0.5]=-1\n",
    "    W2_In=W2_In \n",
    "    \n",
    "    return W1_In, W2_In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESN representation over the dataset\n",
    "\n",
    "The data are splitted because of the limited memory in my gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ESN representation over the dataset\n",
    "def Echo_representations(alpha1, pho1, diluition1, N1, W1_in,\n",
    "                             alpha2, pho2, diluition2, N2, W2_in, gamma12, \n",
    "                             X_tr, X_val, X_test, T_size):\n",
    "\n",
    "    T=np.shape(X_tr)[1]\n",
    "    N_class=np.shape(Y_tr)[1]\n",
    "    \n",
    "    T_conc=np.int(T/T_size)\n",
    "    \n",
    "    rnn=Echo(alpha1, pho1, diluition1, N1, W1_in,\n",
    "                alpha2, pho2, diluition2, N2, W2_in, gamma12, T_conc)\n",
    "\n",
    "    init_state1=tf.placeholder(tf.float32,[None,N1])\n",
    "    init_state2=tf.placeholder(tf.float32,[None,N2])\n",
    "    s=tf.placeholder(tf.float32,[None,T])\n",
    "\n",
    "    states, states1, states2=rnn.train_graph(T,init_state1,init_state2,s,return_all=False)\n",
    "\n",
    "    init=tf.global_variables_initializer()\n",
    "\n",
    "    train_divide=100\n",
    "    N_train_d=int(np.floor(np.shape(Y_tr)[0]/train_divide))\n",
    "\n",
    "    test_divide=50\n",
    "    N_test_d=int(np.floor(np.shape(Y_te)[0]/test_divide))\n",
    "    \n",
    "    val_divide=50\n",
    "    N_val_d=int(np.floor(np.shape(Y_val)[0]/val_divide))\n",
    "\n",
    "    States=np.zeros([N_train_d*train_divide,N,T_size])\n",
    "    States_test=np.zeros([N_test_d*test_divide,N,T_size])\n",
    "    States_val=np.zeros([N_val_d*val_divide,N,T_size])\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(init)\n",
    "        \n",
    "        print('Training Data')\n",
    "\n",
    "        for l in range(train_divide):\n",
    "\n",
    "            images=np.copy(X_tr[l*N_train_d:(l+1)*N_train_d,:])\n",
    "\n",
    "            states_=sess.run(states,feed_dict={init_state1:np.zeros([N_train_d,N1]),init_state2:np.zeros([N_train_d,N2]),s:images})\n",
    "\n",
    "            States[l*N_train_d:(l+1)*N_train_d,:,:]=states_\n",
    "\n",
    "            \n",
    "        print('Testing Data')\n",
    "\n",
    "        for l in range(test_divide):\n",
    "\n",
    "            images=np.copy(X_te[l*N_test_d:(l+1)*N_test_d,:])\n",
    "\n",
    "            states_=sess.run(states,feed_dict={init_state1:np.zeros([N_test_d,N1]),init_state2:np.zeros([N_test_d,N2]),s:images})\n",
    "\n",
    "            States_test[l*N_test_d:(l+1)*N_test_d,:,:]=states_    \n",
    "            \n",
    "       \n",
    "        print('Validating Data')\n",
    "\n",
    "        for l in range(val_divide):\n",
    "\n",
    "            images=np.copy(X_val[l*N_val_d:(l+1)*N_val_d,:])\n",
    "\n",
    "            states_=sess.run(states,feed_dict={init_state1:np.zeros([N_val_d,N1]),init_state2:np.zeros([N_val_d,N2]),s:images})\n",
    "\n",
    "            States_val[l*N_val_d:(l+1)*N_val_d,:,:]=states_\n",
    "        \n",
    "    \n",
    "    return States, States_test, States_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-62ac70059d46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mT_conc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mN_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT_conc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Y_tr' is not defined"
     ]
    }
   ],
   "source": [
    "N_episodes=100000\n",
    "\n",
    "N1=600\n",
    "N2=600\n",
    "N=N1+N2\n",
    "\n",
    "alpha_size=0.0005\n",
    "\n",
    "T_conc=28\n",
    "N_class=np.shape(Y_tr)[1]\n",
    "\n",
    "s=tf.placeholder(tf.float32,[None,N,T_conc])\n",
    "V=tf.reshape(s,[-1,T_conc*N])\n",
    "\n",
    "y_true=tf.placeholder(tf.float32,[None,N_class])\n",
    "\n",
    "W_out=tf.Variable(np.random.uniform(-1,1,[N*T_conc,N_class])/(N),dtype=tf.float32)\n",
    "\n",
    "y=tf.matmul(V,W_out)\n",
    "\n",
    "error=tf.losses.sigmoid_cross_entropy(y_true,y)\n",
    "\n",
    "train=tf.train.AdamOptimizer(learning_rate=alpha_size).minimize(error,var_list=[W_out])\n",
    "\n",
    "batch_size=20            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation  26 , alpha1= 0.020468075714350484  alpha2= 0.18887560283756186\n",
      "Processing Resrvoir Representations: \n",
      "Training Data\n",
      "Testing Data\n",
      "Validating Data\n",
      "Start Training: \n",
      "Iteration:  10000 Reservoir  2 Probability:  0.8095999956130981 Error:  0.1540822833776474\n",
      "Iteration:  30000 Reservoir  2 Probability:  0.8583999872207642 Error:  0.1325637400150299\n",
      "Iteration:  50000 Reservoir  2 Probability:  0.9112000167369843 Error:  0.08820458501577377\n",
      "Iteration:  70000 Reservoir  2 Probability:  0.8930000066757202 Error:  0.11839304864406586\n",
      "Iteration:  90000 Reservoir  2 Probability:  0.9127999842166901 Error:  0.08123615756630898\n",
      "Iteration:  91000 Reservoir  2 Probability:  0.8774000108242035 Error:  0.11494030430912971\n",
      "Iteration:  92000 Reservoir  2 Probability:  0.883400022983551 Error:  0.1177138239145279\n",
      "Iteration:  93000 Reservoir  2 Probability:  0.9214000105857849 Error:  0.07580337300896645\n",
      "Iteration:  94000 Reservoir  2 Probability:  0.9093999862670898 Error:  0.08612212538719177\n",
      "Iteration:  95000 Reservoir  2 Probability:  0.9093999862670898 Error:  0.0908866599202156\n",
      "Iteration:  96000 Reservoir  2 Probability:  0.8404000103473663 Error:  0.15344101935625076\n",
      "Iteration:  97000 Reservoir  2 Probability:  0.9000000059604645 Error:  0.09945215284824371\n",
      "Iteration:  98000 Reservoir  2 Probability:  0.8756000101566315 Error:  0.13966428488492966\n",
      "Iteration:  99000 Reservoir  2 Probability:  0.9181999862194061 Error:  0.0752657800912857\n",
      "Simulation  36 , alpha1= 0.03567399334725241  alpha2= 0.18887560283756186\n",
      "Processing Resrvoir Representations: \n",
      "Training Data\n",
      "Testing Data\n",
      "Validating Data\n",
      "Start Training: \n",
      "Iteration:  10000 Reservoir  3 Probability:  0.8199999928474426 Error:  0.1468724012374878\n",
      "Iteration:  30000 Reservoir  3 Probability:  0.8813999891281128 Error:  0.11579865962266922\n",
      "Iteration:  50000 Reservoir  3 Probability:  0.9269999861717224 Error:  0.07717297598719597\n",
      "Iteration:  70000 Reservoir  3 Probability:  0.9115999937057495 Error:  0.0939176194369793\n",
      "Iteration:  90000 Reservoir  3 Probability:  0.9262000024318695 Error:  0.07768864557147026\n",
      "Iteration:  91000 Reservoir  3 Probability:  0.9257999956607819 Error:  0.07462609186768532\n",
      "Iteration:  92000 Reservoir  3 Probability:  0.9068000018596649 Error:  0.10215168073773384\n",
      "Iteration:  93000 Reservoir  3 Probability:  0.9156000018119812 Error:  0.09574925526976585\n",
      "Iteration:  94000 Reservoir  3 Probability:  0.9167999923229218 Error:  0.08354591950774193\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-d386ba2fb263>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    116\u001b[0m                     \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrand_ind\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m                     \u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[1;31m# If the following condition is true, the program will compute the performance over the test and validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy import io\n",
    "\n",
    "## Simulation \n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "N_episodes=100000                      # Number of training iteration\n",
    "alpha_size=0.0005                      # Learning rate\n",
    "batch_size=20            \n",
    "T_size=28                              # Number of hidden states used for the readout (they are equally spaced across the input sequence)\n",
    "\n",
    "N_class=np.shape(Y_tr)[1]\n",
    "N_size=np.shape(X_tr)[0]\n",
    "\n",
    "N_check=15\n",
    "T_checks=np.concatenate([np.linspace(10000,90000,5),np.linspace(91000,100000,N_check-5)],0)\n",
    "\n",
    "# Number of times the test, validation and training dataset are divided\n",
    "test_divide=2\n",
    "val_divide=2\n",
    "train_divide=10\n",
    "\n",
    "N_test_d=int(np.floor(np.shape(Y_te)[0]/test_divide))\n",
    "N_val_d=int(np.floor(np.shape(Y_val)[0]/val_divide))\n",
    "N_train_d=int(np.floor(np.shape(Y_tr)[0]/train_divide))\n",
    "\n",
    "N_grid=10                               # Number of values of alpha scanned in the loop\n",
    "\n",
    "# Arrays to save the performance\n",
    "p_ra_test=np.zeros([N_grid,N_grid,N_check])\n",
    "p_ra_train=np.zeros([N_grid,N_grid,N_check])\n",
    "p_ra_val=np.zeros([N_grid,N_grid,N_check])\n",
    "\n",
    "## Network parameters\n",
    "\n",
    "N1=600\n",
    "N2=600\n",
    "N=N1+N2\n",
    "\n",
    "N1_av=5                                  # Average number of connections for a node in ESN 1\n",
    "N2_av=3                                  # Average number of connections for a node in ESN 2\n",
    "\n",
    "diluition1=1-N1_av/N1                    # Probability of a zero in the connectivity matrix of ESN 1\n",
    "diluition2=1-N2_av/N2                    # Probability of a zero in the connectivity matrix of ESN 2\n",
    "\n",
    "alpha1=np.exp(np.linspace(-5,0,N_grid))  # Values of alpha 1 scanned\n",
    "alpha2=np.exp(np.linspace(-5,0,N_grid))  # Values of alpha 2 scanned\n",
    "\n",
    "\n",
    "pho1=0.985                               # Value of \\rho for ESN 1\n",
    "pho2=1                                   # Value of \\rho for ESN 2\n",
    "\n",
    "# Connected=True will define a hierarchical structure where ESN 2 feeds into ESN 1\n",
    "# Connected=False will define parallel ESNs\n",
    "Connected=True\n",
    "if Connected==True:\n",
    "    \n",
    "    gamma12=1 # Multiplicative factor of the connectivity between the reservoirs\n",
    "    gamma1=0  # Multiplicative factor of the input connectivity to ESN 1\n",
    "\n",
    "else:\n",
    "    \n",
    "    gamma12=0\n",
    "    gamma1=1\n",
    "    \n",
    "## Training    \n",
    "        \n",
    "for i in range(2,N_grid):\n",
    "\n",
    "    for j in range(6,7):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        index_help=0\n",
    "\n",
    "        W1_in,W2_in=W_input(N1,N2)\n",
    "        W1_in=gamma1*W1_in.T\n",
    "\n",
    "        print('Simulation ', i*N_grid+j, ', alpha1=',alpha1[i], ' alpha2=', alpha2[j])\n",
    "        print('Processing Resrvoir Representations: ')\n",
    "\n",
    "        States, States_test, States_val=Echo_representations(alpha1[i], pho1, diluition1, N1, W1_in,\n",
    "                                 alpha2[j], pho2, diluition2, N2, W2_in, gamma12, \n",
    "                                 X_tr, X_val, X_te, T_size)\n",
    "\n",
    "        print('Start Training: ')\n",
    "        \n",
    "        # Definition of the graph fpr the training\n",
    "        # The placeholder s receives the representations of the ESN used for the readout (for the specific batch_size sequences considered) \n",
    "        s=tf.placeholder(tf.float32,[None,N,T_size])\n",
    "        V=tf.reshape(s,[-1,T_size*N])\n",
    "\n",
    "        y_true=tf.placeholder(tf.float32,[None,N_class])\n",
    "\n",
    "        W_out=tf.Variable(np.random.uniform(-1,1,[N*T_size,N_class])/(N),dtype=tf.float32)\n",
    "\n",
    "        y=tf.matmul(V,W_out)\n",
    "\n",
    "        error=tf.losses.sigmoid_cross_entropy(y_true,y)\n",
    "\n",
    "        train=tf.train.AdamOptimizer(learning_rate=alpha_size).minimize(error,var_list=[W_out])\n",
    "        \n",
    "        init=tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "\n",
    "            for n in range(N_episodes):\n",
    "\n",
    "                if n>0:\n",
    "\n",
    "                    rand_ind=np.random.randint(0,N_size,(batch_size,))\n",
    "\n",
    "                    images=States[rand_ind,:,:]\n",
    "\n",
    "                    labels=Y_tr[rand_ind,:]\n",
    "\n",
    "                    _=sess.run(train,feed_dict={y_true:labels,s:images})\n",
    "\n",
    "                # If the following condition is true, the program will compute the performance over the test and validation data\n",
    "                if n==T_checks[index_help]:\n",
    "\n",
    "                    index_help=index_help+1\n",
    "\n",
    "                    matches=tf.equal(tf.argmax(y_true,1),tf.argmax(y,1))\n",
    "                    p=tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "    \n",
    "                    # Performance on the test data, computed as a mean over the different splits of the dataset through the variable p_ra_ and error_ \n",
    "                    p_ra_=0\n",
    "                    error_=0\n",
    "\n",
    "                    for l in range(test_divide):\n",
    "\n",
    "                        images=States_test[l*N_test_d:(l+1)*N_test_d,:,:]\n",
    "                        labels=Y_te[l*N_test_d:(l+1)*N_test_d,:]\n",
    "\n",
    "                        p_ra1_,error1_=sess.run([p,error],feed_dict={y_true:labels,s:images})\n",
    "\n",
    "                        p_ra_=p_ra_+p_ra1_/test_divide\n",
    "                        error_=error_+error1_/test_divide\n",
    "\n",
    "\n",
    "                    p_ra_test[i,j,index_help]=p_ra_\n",
    "\n",
    "                    #print('Iteration: ',n,'Reservoir ',i,'Probability: ',p_ra_,'Error: ',error_)\n",
    "\n",
    "                    # Performance on the validation data, computed as a mean over the different splits of the dataset through the variable p_ra_ and error_ \n",
    "                    p_ra_v=0\n",
    "                    error_v=0\n",
    "\n",
    "                    for l in range(val_divide):\n",
    "\n",
    "                        images=States_val[l*N_val_d:(l+1)*N_val_d,:,:]\n",
    "                        labels=Y_val[l*N_val_d:(l+1)*N_val_d,:]\n",
    "\n",
    "                        p_ra3_,error3_=sess.run([p,error],feed_dict={y_true:labels,s:images})\n",
    "\n",
    "                        p_ra_v=p_ra_v+p_ra3_/val_divide\n",
    "                        error_v=error_v+error3_/val_divide\n",
    "\n",
    "                    p_ra_val[i,j,index_help]=p_ra_v\n",
    "\n",
    "                    print('Iteration: ',n,'Reservoir ',i,'Probability: ',p_ra_v,'Error: ',error_v)\n",
    "\n",
    "\n",
    "            #io.savemat(\"Acc_test_check\"+repr(i)+\"_\"+repr(j)+\".mat\",{\"array\": p_ra_test[i,j,:]})\n",
    "            #io.savemat(\"Acc_val_check\"+repr(i)+\"_\"+repr(j)+\".mat\",{\"array\": p_ra_val[i,j,:]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13.1",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
