{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.client import timeline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-ba792ace9d17>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\lucam\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\lucam\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\lucam\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\lucam\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\lucam\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cadf0c4cf8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_tr=np.reshape(mnist.train.images,[-1,28,28])\n",
    "Y_tr=mnist.train.labels\n",
    "X_te=np.reshape(mnist.test.images,[-1,28,28])\n",
    "Y_te=mnist.test.labels\n",
    "X_val=np.reshape(mnist.validation.images,[-1,28,28])\n",
    "Y_val=mnist.validation.labels\n",
    "plt.imshow(X_tr[2,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for Catastrophic forgetting simulation\n",
    "The following function defines a list of indexes, where each group of indexes in the list refers to the indexes of the dataset\n",
    "that belongs to a 'task'. The tasks are the set of data that will be played sequentially in the catastrophic forgetting \n",
    "simulation, where the network is trained over the data of a task only once.\n",
    "In this case, the digits belonging to the first 5 classes compose the first task, while the remaining data compose 5 tasks, one for each digit. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Data for catastrophic\n",
    "\n",
    "\n",
    "def Data_Catastrophic(Y_tr,Y_te,Y_val):\n",
    "\n",
    "    Y_tr_class=np.argwhere(Y_tr==1)\n",
    "    Y_te_class=np.argwhere(Y_te==1)\n",
    "    Y_val_class=np.argwhere(Y_val==1)\n",
    "\n",
    "    N_calss=np.shape(Y_tr,1)\n",
    "    \n",
    "    digits_perm=np.random.permutation(N_class)\n",
    "\n",
    "    tr1=[]\n",
    "    te1=[]\n",
    "\n",
    "    tr2=[]\n",
    "    te2=[]\n",
    "\n",
    "    val1=[]\n",
    "    val2=[]\n",
    "\n",
    "    for i in range(N_class):\n",
    "\n",
    "        if i<np.int(N_class/2):\n",
    "\n",
    "            indexes=np.squeeze(np.argwhere(Y_tr_class[:,1]==digits_perm[i]))\n",
    "\n",
    "            tr1=np.concatenate([tr1,np.int32(indexes)],axis=0)\n",
    "\n",
    "            indexes=np.squeeze(np.argwhere(Y_te_class[:,1]==digits_perm[i]))\n",
    "\n",
    "            te1=np.concatenate([te1,np.int32(indexes)],axis=0)\n",
    "\n",
    "            indexes=np.squeeze(np.argwhere(Y_val_class[:,1]==digits_perm[i]))\n",
    "\n",
    "            val1=np.concatenate([val1,np.int32(indexes)],axis=0)\n",
    "\n",
    "\n",
    "        if i>=np.int(N_class/2):\n",
    "\n",
    "            indexes=np.squeeze(np.argwhere(Y_tr_class[:,1]==digits_perm[i]))\n",
    "\n",
    "            tr2.append(np.int32(indexes))\n",
    "\n",
    "            indexes=np.squeeze(np.argwhere(Y_te_class[:,1]==digits_perm[i]))\n",
    "\n",
    "            te2.append(np.int32(indexes))\n",
    "\n",
    "            indexes=np.squeeze(np.argwhere(Y_val_class[:,1]==digits_perm[i]))\n",
    "\n",
    "            val2.append(np.int32(indexes))\n",
    "\n",
    "\n",
    "\n",
    "    tr1=np.int32(tr1)\n",
    "    te1=np.int32(te1)\n",
    "    val1=np.int32(val1)\n",
    "\n",
    "    tr=[]\n",
    "    te=[]\n",
    "    val=[]\n",
    "\n",
    "    tr.append(tr1)\n",
    "    te.append(te1)\n",
    "    val.append(val1)\n",
    "\n",
    "    for i in range(np.int(N_class/2)):\n",
    "\n",
    "        tr.append(tr2[i])\n",
    "        te.append(te2[i])\n",
    "        val.append(val2[i])\n",
    "        \n",
    "        N_tr_check=0\n",
    "        N_te_check=0\n",
    "        N_te=np.zeros(6)\n",
    "        N_val=np.zeros(6)\n",
    "\n",
    "    for i in range(np.int(N_class/2)+1):\n",
    "\n",
    "        N_te[i]=np.shape(te[i])[0]\n",
    "        N_val[i]=np.shape(val[i])[0]\n",
    "\n",
    "        N_tr_check+=np.shape(tr[i])[0]\n",
    "        N_te_check+=np.shape(te[i])[0]\n",
    "\n",
    "    return tr, te, val, N_te, N_val\n",
    "    \n",
    "tr, te, val, N_te, N_val=Data_Catastrophic(Y_tr,Y_te,Y_val)   \n",
    "\n",
    "\n",
    "# Check\n",
    "\n",
    "N_tr_check=0\n",
    "N_te_check=0\n",
    "N_te=np.zeros(6)\n",
    "N_val=np.zeros(6)\n",
    "\n",
    "for i in range(6):\n",
    "    \n",
    "    N_te[i]=np.shape(te[i])[0]\n",
    "    N_val[i]=np.shape(val[i])[0]\n",
    "    \n",
    "    N_tr_check+=np.shape(tr[i])[0]\n",
    "    N_te_check+=np.shape(te[i])[0]\n",
    "    \n",
    "print(N_tr_check)\n",
    "print(N_te_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESN, definition of the graph for the computation of the activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Echo:\n",
    "    \n",
    "    def __init__(self,dt,tau_m,tau_M,diluition,N,W_in):\n",
    "            \n",
    "            self.dt=dt\n",
    "            self.N=N\n",
    "            self.tau_m=tau_m\n",
    "            self.tau_M=tau_M\n",
    "            self.Diluition=diluition\n",
    "            \n",
    "            W_np=np.random.uniform(-1,1,[N,N])\n",
    "            D=np.random.uniform(0,1,(N,N))>np.ones((N,N))*diluition\n",
    "            W_np=W_np*D.astype(int)\n",
    "            \n",
    "            eig=np.linalg.eigvals(W_np)\n",
    "            self.eig=eig\n",
    "            \n",
    "            alpha_np=dt/(2*tau_m)\n",
    "            pho_np=1-2*tau_m/tau_M\n",
    "            \n",
    "            W_np=pho_np*W_np/(np.max(np.absolute(eig)))\n",
    "            \n",
    "            self.W_np=W_np\n",
    "            self.alpha_np=alpha_np\n",
    "            self.pho_np=pho_np\n",
    "            \n",
    "            self.W=tf.Variable(W_np,trainable=False,dtype=tf.float32)\n",
    "            self.alpha=tf.constant(alpha_np,dtype=tf.float32)\n",
    "            \n",
    "            self.W_in_np=W_in\n",
    "            self.W_in=tf.Variable(W_in,trainable=False,dtype=tf.float32)\n",
    "            self.N_in=np.shape(W_in)[0]\n",
    "            \n",
    "            \n",
    "    def evolution_graph(self,T,init_state,inputs):\n",
    "\n",
    "        \n",
    "        state_hidden=init_state\n",
    "        states_hidden=[]\n",
    "        xs=[]\n",
    "        \n",
    "        \n",
    "        for t in range(T):\n",
    "            \n",
    "            prev_state=tf.identity(state_hidden)\n",
    "            state_hidden=( (1-self.alpha)*prev_state+self.alpha*tf.tanh( tf.matmul(prev_state,self.W)+tf.matmul(inputs[:,:,t],self.W_in) ))\n",
    "            states_hidden.append(state_hidden)\n",
    "            \n",
    "        states=tf.concat([tf.expand_dims(s,2) for s in states_hidden],2)\n",
    "\n",
    "    \n",
    "        return states, state_hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_input(N_proj,N,n_mean_conn):\n",
    "\n",
    "    W_In=np.random.randn(N,N_proj)\n",
    "\n",
    "    return W_In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the specificty metric \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(state,labels,N_e,in_task):\n",
    "    \n",
    "    if in_task==True:\n",
    "    \n",
    "        N_class=np.shape(labels)[1]\n",
    "        N=np.shape(state)[1]\n",
    "\n",
    "        activation_class=np.zeros([N,N_class])\n",
    "        \n",
    "        for i in range(N_class):\n",
    "\n",
    "            activity_class=state*np.tile(np.expand_dims(labels[:,i],1),[1,N])\n",
    "\n",
    "            activation_class[:,i]=np.transpose(np.sum((activity_class>0),0))        \n",
    "\n",
    "    else:\n",
    "        \n",
    "        N_class=np.shape(state)[0]\n",
    "        N=np.shape(state[0])[1]\n",
    "        \n",
    "        activation_class=np.zeros([N,N_class])\n",
    "\n",
    "        for i in range(N_class):\n",
    "\n",
    "            activation_class[:,i]=np.transpose(np.sum((state[i]>0),0))        \n",
    "        \n",
    "        \n",
    "    spec_tensor=np.zeros([N,N_class,N_class])\n",
    "    spec_tensor1=np.zeros([N,N_class,N_class])\n",
    "\n",
    "    spec=np.zeros([N])\n",
    "    spec1=np.zeros([N])\n",
    "    \n",
    "    \n",
    "    spec_new=np.zeros([N])\n",
    "    spec1_new=np.zeros([N])\n",
    "    \n",
    "    overlap=np.zeros([N,N_class])\n",
    "    overlap1=np.zeros([N,N_class])\n",
    "    \n",
    "    C_active=np.sum(activation_class>0,1)\n",
    "    \n",
    "    index_=np.sum(activation_class,1)>0\n",
    "    \n",
    "    for i in range(N_class):\n",
    "        \n",
    "        overlap[:,i]=activation_class[:,i]/N_e[i]\n",
    "        \n",
    "        overlap1[index_,i]=(activation_class[index_,i]/N_e[i])/np.max(activation_class[index_,:]/np.tile(N_e,[int(np.sum(index_)),1]),1)\n",
    "        \n",
    "        \n",
    "        for j in range(N_class):\n",
    "            \n",
    "            activation_den=activation_class[:,i]+activation_class[:,j]>0\n",
    "                        \n",
    "            spec_tensor1[:,i,j]=np.absolute(activation_class[:,i]/N_e[i]-activation_class[:,j]/N_e[j])\n",
    "            \n",
    "            spec_tensor[activation_den,i,j]=spec_tensor1[activation_den,i,j]/(activation_class[activation_den,i]/N_e[i]+activation_class[activation_den,j]/N_e[j])\n",
    "            \n",
    "    for i in range(N):\n",
    "        \n",
    "        if np.sum(spec_tensor[i,:,:]>0)>0:\n",
    "            \n",
    "            spec[i]=np.mean(spec_tensor[i,spec_tensor[i,:,:]>0])\n",
    "            spec1[i]=np.mean(spec_tensor1[i,spec_tensor[i,:,:]>0])\n",
    "            \n",
    "            \n",
    "    return spec, spec1, overlap, overlap1, C_active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESN representation over the dataset\n",
    "\n",
    "The data are splitted because of the limited memory in my gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compute_states(N,dt,T,tau_m,tau_M,diluition,N_proj):\n",
    "    \n",
    "    W_in=np.random.randn(N,N_proj)\n",
    "    W_in=0.1*W_in.T\n",
    "\n",
    "    rnn=Echo(dt,tau_m,tau_M,diluition,N,W_in)\n",
    "\n",
    "    init_state=tf.placeholder(tf.float32,[None,N])\n",
    "    s=tf.placeholder(tf.float32,[None,N_proj,T])\n",
    "\n",
    "    states,state=rnn.evolution_graph(T,init_state,s)\n",
    "\n",
    "    init=tf.global_variables_initializer()\n",
    "\n",
    "    train_divide=20\n",
    "    N_train_d=int(np.floor(np.shape(Y_tr)[0]/train_divide))\n",
    "\n",
    "    Data_train=np.zeros([np.shape(Y_tr)[0],N,T])\n",
    "\n",
    "    test_divide=10\n",
    "    N_test_d=int(np.floor(np.shape(Y_te)[0]/test_divide))\n",
    "\n",
    "    Data_test=np.zeros([np.shape(Y_te)[0],N,T])\n",
    "    \n",
    "    val_divide=10\n",
    "    N_val_d=int(np.floor(np.shape(Y_val)[0]/val_divide))\n",
    "\n",
    "    Data_val=np.zeros([np.shape(Y_val)[0],N,T])\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(init)\n",
    "\n",
    "        print('Computing States for the training set')\n",
    "\n",
    "        for l in range(train_divide):\n",
    "\n",
    "            images=X_tr[l*N_train_d:(l+1)*N_train_d,:,:]\n",
    "            labels=Y_tr[l*N_train_d:(l+1)*N_train_d,:]\n",
    "\n",
    "            states_=sess.run(states,feed_dict={init_state:np.zeros([N_train_d,N]),s:images})\n",
    "\n",
    "            Data_train[l*N_train_d:(l+1)*N_train_d,:,:]=states_\n",
    "\n",
    "        print('Computing States for the test set')\n",
    "\n",
    "        for l in range(test_divide):\n",
    "\n",
    "            images=X_te[l*N_test_d:(l+1)*N_test_d,:,:]\n",
    "            labels=Y_te[l*N_test_d:(l+1)*N_test_d,:]\n",
    "\n",
    "            states_=sess.run(states,feed_dict={init_state:np.zeros([N_test_d,N]),s:images})\n",
    "\n",
    "            Data_test[l*N_test_d:(l+1)*N_test_d,:,:]=states_\n",
    "            \n",
    "        print('Computing States for the validation set')\n",
    "\n",
    "        for l in range(val_divide):\n",
    "\n",
    "            images=X_val[l*N_val_d:(l+1)*N_val_d,:,:]\n",
    "            labels=Y_val[l*N_val_d:(l+1)*N_val_d,:]\n",
    "\n",
    "            states_=sess.run(states,feed_dict={init_state:np.zeros([N_val_d,N]),s:images})\n",
    "\n",
    "            Data_val[l*N_val_d:(l+1)*N_val_d,:,:]=states_\n",
    "            \n",
    "    return Data_train, Data_test, Data_val\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow graph for the training\n",
    "\n",
    "The following class handles the training of 2 possible models:\n",
    "\n",
    "<ul>\n",
    "    <li> SpaRCe: training of N_copies NNs starting from different initial conditions, i.e. different starting sparsity level.            The algorithm is composed by an initialisation phase (it initialises the starting value of the thresholds, theta_g)            and a training phase (where the thresholds theta_i and the output weights W_out are optimised). The thresholds are              theta=theta_g+theta_i, where the first factor corresponds to the initialisation, and the second factor is optimised            (this split is due to convenience, to have the possibility to check each factor separately)  </li>\n",
    "    <li> ESN: Training of the readout of an ESN. It is possible to compute the performance as the learning rate varies by                selecting scan=True. </li>\n",
    " </ul>\n",
    " \n",
    "The hyperparameters suggested are already in a good configuration. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Models:\n",
    "    \n",
    "    def __init__(self,N,N_class,alpha_size,batch_size,N_ep):\n",
    "        \n",
    "        self.T=28\n",
    "        \n",
    "        self.N=N\n",
    "        \n",
    "        self.N_class=N_class\n",
    "        \n",
    "        self.alpha_size=alpha_size\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.N_episodes=N_ep\n",
    "        \n",
    "        \n",
    "    def ESN(self,state,y_true,scan,N_copies):\n",
    "        \n",
    "        if scan==False:\n",
    "    \n",
    "            alpha_sizes=[self.alpha_size]\n",
    "                               \n",
    "        \n",
    "        if scan==True:\n",
    "            \n",
    "            alpha_sizes=0.0005*2**(-np.linspace(0,4,5))\n",
    "            \n",
    "        W_out=[]\n",
    "        y=[]\n",
    "        error=[]\n",
    "        train=[]\n",
    "\n",
    "        for i in range(N_copies):\n",
    "\n",
    "            W_out.append(tf.Variable(np.random.uniform(-1,1,[self.N*self.T,self.N_class])/(self.N/10),dtype=tf.float32))\n",
    "\n",
    "            y.append( tf.matmul(state,W_out[i]) )\n",
    "\n",
    "            error.append(tf.losses.sigmoid_cross_entropy(y_true,y[i]))\n",
    "\n",
    "            train.append(tf.train.AdamOptimizer(learning_rate=alpha_sizes[i]).minimize(error[i],var_list=[W_out[i]]))\n",
    "\n",
    "\n",
    "        return y, error, train \n",
    "    \n",
    "    def SpaRCe(self,state,y_true,theta_g_start):\n",
    "        \n",
    "        theta_g=[]\n",
    "        theta_i=[]\n",
    "        W_out=[]\n",
    "        state_sparse=[]\n",
    "        y=[]\n",
    "        error=[]\n",
    "        train1=[]\n",
    "        train2=[]\n",
    "                \n",
    "        theta_istart=np.random.randn(N,self.T)/N\n",
    "        theta_istart=np.reshape(theta_istart,[-1,N*self.T])\n",
    "\n",
    "        for i in range(N_copies):\n",
    "\n",
    "            theta_g.append(tf.Variable(theta_g_start[i], trainable=False, dtype=tf.float32))\n",
    "\n",
    "            theta_i.append(tf.Variable(theta_istart,dtype=tf.float32))\n",
    "\n",
    "            W_out.append(tf.Variable(np.random.uniform(0,1,[self.N*self.T,N_class])/(N),dtype=tf.float32))\n",
    "            \n",
    "            state_sparse.append(tf.sign(state)*tf.nn.relu(tf.abs(state)-theta_g[i]-theta_i[i]))     \n",
    "\n",
    "            y.append(tf.matmul(state_sparse[i],W_out[i]))\n",
    "\n",
    "            error.append(tf.losses.sigmoid_cross_entropy(y_true,y[i]))\n",
    "    \n",
    "            train1.append(tf.train.AdamOptimizer(learning_rate=alpha_size).minimize(error[i],var_list=[W_out[i]]))\n",
    "            train2.append(tf.train.AdamOptimizer(learning_rate=alpha_size/10).minimize(error[i],var_list=[theta_i[i]]))\n",
    "\n",
    "            train=train1+train2\n",
    "        \n",
    "        \n",
    "        return state_sparse, y, error, train\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING\n",
    "\n",
    "It is possible to specify the model to train (SpaRCe or readout of ESN).\n",
    "The network performance across the whole dataset will be computed after each task (the group of data that are experienced\n",
    "sequentially). The vectors of performance and specificity metrics contains the accuracies for the different tasks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing States for the training set\n",
      "Computing States for the test set\n",
      "Computing States for the validation set\n",
      "INITIALISATION\n",
      "TRAINING\n",
      "1\n",
      "Test:  Iteration:  1 Reservoir  0 Probability:  [0.44187474 0.09689923 0.         0.         0.         0.        ]\n",
      "Test:  Iteration:  1 Reservoir  1 Probability:  [0.23986901 0.         0.02675917 0.         0.         0.90918583]\n",
      "Test:  Iteration:  1 Reservoir  2 Probability:  [0.18419975 0.         0.06739346 0.         0.         0.        ]\n",
      "Test:  Iteration:  1 Reservoir  3 Probability:  [0.07961523 0.01453488 0.         0.27753305 0.91836733 0.        ]\n",
      "Test:  Iteration:  1 Reservoir  4 Probability:  [0.09312321 0.0377907  0.         0.         0.         0.90918583]\n",
      "1000\n",
      "Test:  Iteration:  1000 Reservoir  0 Probability:  [0.92754811 0.         0.         0.         0.         0.        ]\n",
      "Test:  Iteration:  1000 Reservoir  1 Probability:  [0.91772413 0.         0.         0.         0.         0.        ]\n",
      "Test:  Iteration:  1000 Reservoir  2 Probability:  [0.91363078 0.         0.         0.         0.         0.        ]\n",
      "Test:  Iteration:  1000 Reservoir  3 Probability:  [0.90728611 0.         0.         0.         0.         0.        ]\n",
      "Test:  Iteration:  1000 Reservoir  4 Probability:  [0.88743347 0.         0.         0.         0.         0.        ]\n",
      "1067\n",
      "Test:  Iteration:  1067 Reservoir  0 Probability:  [0. 1. 0. 0. 0. 0.]\n",
      "Test:  Iteration:  1067 Reservoir  1 Probability:  [0. 1. 0. 0. 0. 0.]\n",
      "Test:  Iteration:  1067 Reservoir  2 Probability:  [0. 1. 0. 0. 0. 0.]\n",
      "Test:  Iteration:  1067 Reservoir  3 Probability:  [0.03069996 1.         0.         0.         0.         0.        ]\n",
      "Test:  Iteration:  1067 Reservoir  4 Probability:  [0.14797381 0.99709302 0.         0.         0.         0.        ]\n",
      "1134\n",
      "Test:  Iteration:  1134 Reservoir  0 Probability:  [0. 0. 1. 0. 0. 0.]\n",
      "Test:  Iteration:  1134 Reservoir  1 Probability:  [0.         0.00484496 1.         0.         0.         0.        ]\n",
      "Test:  Iteration:  1134 Reservoir  2 Probability:  [0.         0.03488372 1.         0.         0.         0.        ]\n",
      "Test:  Iteration:  1134 Reservoir  3 Probability:  [0.         0.21414728 1.         0.         0.         0.        ]\n",
      "Test:  Iteration:  1134 Reservoir  4 Probability:  [0.         0.47577518 1.         0.         0.         0.        ]\n",
      "1201\n",
      "Test:  Iteration:  1201 Reservoir  0 Probability:  [0. 0. 0. 1. 0. 0.]\n",
      "Test:  Iteration:  1201 Reservoir  1 Probability:  [0. 0. 0. 1. 0. 0.]\n",
      "Test:  Iteration:  1201 Reservoir  2 Probability:  [0. 0. 0. 1. 0. 0.]\n",
      "Test:  Iteration:  1201 Reservoir  3 Probability:  [0.         0.         0.00198216 1.         0.         0.        ]\n",
      "Test:  Iteration:  1201 Reservoir  4 Probability:  [0.00757266 0.         0.06640238 1.         0.         0.        ]\n",
      "1268\n",
      "Test:  Iteration:  1268 Reservoir  0 Probability:  [0. 0. 0. 0. 1. 0.]\n",
      "Test:  Iteration:  1268 Reservoir  1 Probability:  [0. 0. 0. 0. 1. 0.]\n",
      "Test:  Iteration:  1268 Reservoir  2 Probability:  [0. 0. 0. 0. 1. 0.]\n",
      "Test:  Iteration:  1268 Reservoir  3 Probability:  [0. 0. 0. 0. 1. 0.]\n",
      "Test:  Iteration:  1268 Reservoir  4 Probability:  [0. 0. 0. 0. 1. 0.]\n",
      "1335\n",
      "Test:  Iteration:  1335 Reservoir  0 Probability:  [0. 0. 0. 0. 0. 1.]\n",
      "Test:  Iteration:  1335 Reservoir  1 Probability:  [0. 0. 0. 0. 0. 1.]\n",
      "Test:  Iteration:  1335 Reservoir  2 Probability:  [0. 0. 0. 0. 0. 1.]\n",
      "Test:  Iteration:  1335 Reservoir  3 Probability:  [0.         0.         0.         0.         0.00102041 1.        ]\n",
      "Test:  Iteration:  1335 Reservoir  4 Probability:  [0.03356529 0.         0.         0.         0.04795918 1.        ]\n",
      "Computing States for the training set\n",
      "Computing States for the test set\n",
      "Computing States for the validation set\n",
      "INITIALISATION\n",
      "TRAINING\n",
      "1\n",
      "Test:  Iteration:  1 Reservoir  0 Probability:  [0.49045762 0.         0.         0.00097276 0.02079208 0.        ]\n",
      "Test:  Iteration:  1 Reservoir  1 Probability:  [0.48573774 0.         0.         0.04766537 0.0029703  0.        ]\n",
      "Test:  Iteration:  1 Reservoir  2 Probability:  [0.18694849 0.11020408 0.         0.00875486 0.0029703  0.        ]\n",
      "Test:  Iteration:  1 Reservoir  3 Probability:  [0.13687667 0.04387755 0.51232034 0.2344358  0.         0.        ]\n",
      "Test:  Iteration:  1 Reservoir  4 Probability:  [0.20562282 0.08163265 0.25359342 0.00972763 0.03762376 0.        ]\n",
      "1000\n",
      "Test:  Iteration:  1000 Reservoir  0 Probability:  [0.93638414 0.         0.         0.         0.         0.        ]\n",
      "Test:  Iteration:  1000 Reservoir  1 Probability:  [0.93494767 0.         0.         0.         0.         0.        ]\n",
      "Test:  Iteration:  1000 Reservoir  2 Probability:  [0.93043298 0.         0.         0.         0.         0.        ]\n",
      "Test:  Iteration:  1000 Reservoir  3 Probability:  [0.92242974 0.         0.         0.         0.         0.        ]\n",
      "Test:  Iteration:  1000 Reservoir  4 Probability:  [0.90888572 0.         0.         0.         0.         0.        ]\n",
      "1067\n",
      "Test:  Iteration:  1067 Reservoir  0 Probability:  [0. 1. 0. 0. 0. 0.]\n",
      "Test:  Iteration:  1067 Reservoir  1 Probability:  [0. 1. 0. 0. 0. 0.]\n",
      "Test:  Iteration:  1067 Reservoir  2 Probability:  [0. 1. 0. 0. 0. 0.]\n",
      "Test:  Iteration:  1067 Reservoir  3 Probability:  [0.01046583 1.         0.         0.         0.         0.        ]\n",
      "Test:  Iteration:  1067 Reservoir  4 Probability:  [0.07100349 1.         0.         0.         0.         0.        ]\n",
      "1134\n",
      "Test:  Iteration:  1134 Reservoir  0 Probability:  [0. 0. 1. 0. 0. 0.]\n",
      "Test:  Iteration:  1134 Reservoir  1 Probability:  [0. 0. 1. 0. 0. 0.]\n",
      "Test:  Iteration:  1134 Reservoir  2 Probability:  [0. 0. 1. 0. 0. 0.]\n",
      "Test:  Iteration:  1134 Reservoir  3 Probability:  [0. 0. 1. 0. 0. 0.]\n",
      "Test:  Iteration:  1134 Reservoir  4 Probability:  [0.00184691 0.         1.         0.         0.         0.        ]\n",
      "1201\n",
      "Test:  Iteration:  1201 Reservoir  0 Probability:  [0. 0. 0. 1. 0. 0.]\n",
      "Test:  Iteration:  1201 Reservoir  1 Probability:  [0. 0. 0. 1. 0. 0.]\n",
      "Test:  Iteration:  1201 Reservoir  2 Probability:  [0. 0. 0. 1. 0. 0.]\n",
      "Test:  Iteration:  1201 Reservoir  3 Probability:  [0.00102606 0.         0.         1.         0.         0.        ]\n",
      "Test:  Iteration:  1201 Reservoir  4 Probability:  [0.06443669 0.         0.01232033 0.9951362  0.         0.        ]\n",
      "1268\n",
      "Test:  Iteration:  1268 Reservoir  0 Probability:  [0. 0. 0. 0. 1. 0.]\n",
      "Test:  Iteration:  1268 Reservoir  1 Probability:  [0. 0. 0. 0. 1. 0.]\n",
      "Test:  Iteration:  1268 Reservoir  2 Probability:  [0.        0.        0.        0.0155642 1.        0.       ]\n",
      "Test:  Iteration:  1268 Reservoir  3 Probability:  [2.05212389e-04 0.00000000e+00 0.00000000e+00 7.39299580e-02\n",
      " 1.00000000e+00 0.00000000e+00]\n",
      "Test:  Iteration:  1268 Reservoir  4 Probability:  [0.00451467 0.         0.         0.31420234 0.99900991 0.        ]\n",
      "1335\n",
      "Test:  Iteration:  1335 Reservoir  0 Probability:  [0. 0. 0. 0. 0. 1.]\n",
      "Test:  Iteration:  1335 Reservoir  1 Probability:  [0. 0. 0. 0. 0. 1.]\n",
      "Test:  Iteration:  1335 Reservoir  2 Probability:  [0.        0.        0.        0.0077821 0.        1.       ]\n",
      "Test:  Iteration:  1335 Reservoir  3 Probability:  [0.         0.         0.         0.03015564 0.         1.        ]\n",
      "Test:  Iteration:  1335 Reservoir  4 Probability:  [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.04085602e-01\n",
      " 9.90099041e-04 1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "from scipy import io\n",
    "\n",
    "N=1000                                        \n",
    "dt=0.01\n",
    "tau_m=0.03\n",
    "tau_M=2\n",
    "diluition=0.99\n",
    "\n",
    "T=np.shape(X_tr)[2]\n",
    "N_class=np.shape(Y_tr)[1]\n",
    "N_proj=np.shape(X_tr)[1]\n",
    "\n",
    "N_train=np.shape(X_tr)[0]\n",
    "\n",
    "N_check=6\n",
    "\n",
    "n_switch=np.zeros(6)\n",
    "n_switch[0]=1000\n",
    "n_switch[1:6]=n_switch[0]+np.round(n_switch[0]/15)*np.arange(1,6)\n",
    "n_switch=np.int32(n_switch)\n",
    "N_episodes=n_switch[5]\n",
    "\n",
    "N_switch=6\n",
    "  \n",
    "N_repetitions=10\n",
    "\n",
    "MODEL=2\n",
    "    \n",
    "for k in range(0,2):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    tr, te, val, N_te, N_val=Data_Catastrophic(Y_tr,Y_te,Y_val)    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        Data_train, Data_test, Data_val=Compute_states(N,dt,T,tau_m,tau_M,diluition,N_proj)\n",
    "\n",
    "        print('INITIALISATION')\n",
    "\n",
    "        if MODEL==1:\n",
    "\n",
    "            alpha_size=0.0005                                                  # Learning rate\n",
    "            batch_size=20    \n",
    "            NN_model=Models(N,N_class,alpha_size,batch_size,N_episodes)\n",
    "\n",
    "            theta_g_start=[]\n",
    "\n",
    "            N_copies=10\n",
    "\n",
    "            for n in range(N_copies):\n",
    "\n",
    "                theta_g_start_help=np.zeros([N,T])\n",
    "\n",
    "                for t in range(T):\n",
    "\n",
    "                    state_help=Data_train[:,:,t]\n",
    "\n",
    "                    theta_g_start_help[:,t]=np.percentile(np.abs(state_help),100-N_copies+n,0)\n",
    "\n",
    "                theta_g_start.append(np.reshape(theta_g_start_help,[-1,T*N]))\n",
    "\n",
    "            States_tr=np.reshape(Data_train,[-1,N*T])\n",
    "            States_te=np.reshape(Data_test,[-1,N*T])\n",
    "            States_val=np.reshape(Data_val,[-1,N*T])\n",
    "\n",
    "            s=tf.placeholder(tf.float32,[None,N*T])\n",
    "            y_true=tf.placeholder(tf.float32,[None,N_class])\n",
    "\n",
    "            state_sparse, y, error, train = NN_model.SpaRCe(s,y_true,theta_g_start)\n",
    "\n",
    "        if MODEL==2:\n",
    "\n",
    "            alpha_size=0.0005                                               # Learning rate\n",
    "            batch_size=20\n",
    "\n",
    "            N_copies=5\n",
    "            scan=True\n",
    "\n",
    "            if scan==False:\n",
    "                N_copies=1\n",
    "\n",
    "            NN_model=Models(N,N_class,alpha_size,batch_size,N_episodes)\n",
    "\n",
    "            States_tr=np.reshape(Data_train,[-1,N*T])\n",
    "            States_te=np.reshape(Data_test,[-1,N*T])\n",
    "            States_val=np.reshape(Data_val,[-1,N*T])\n",
    "\n",
    "            s=tf.placeholder(tf.float32,[None,N*T])\n",
    "            y_true=tf.placeholder(tf.float32,[None,N_class])\n",
    "\n",
    "            y, error, train = NN_model.ESN(s,y_true,scan,N_copies)\n",
    "\n",
    "\n",
    "        init=tf.global_variables_initializer()\n",
    "\n",
    "        sess.run(init)\n",
    "\n",
    "        index_help=0\n",
    "\n",
    "        print('TRAINING')\n",
    "\n",
    "        pra_save=np.zeros([N_copies,N_switch,N_check+1])\n",
    "        cl_save=np.zeros([N_copies,N_switch,N_check+1])\n",
    "\n",
    "        overlap_save=np.zeros([N_copies,N*T,N_switch,N_check+1])\n",
    "        overlap1_save=np.zeros([N_copies,N*T,N_switch,N_check+1])\n",
    "        C_active_save=np.zeros([N_copies,N*T,N_check+1])\n",
    "        sp_tasks_save=np.zeros([N_copies,N*T,N_check+1])\n",
    "        sp1_tasks_save=np.zeros([N_copies,N*T,N_check+1])\n",
    "\n",
    "        for n in range(1,N_episodes+1):      \n",
    "\n",
    "\n",
    "            E_current=np.sum(np.int32(np.floor((n-1)/n_switch)>0))\n",
    "\n",
    "            rand_ind=np.random.randint(0,np.shape(tr[E_current])[0],(batch_size,))\n",
    "\n",
    "            images=States_tr[tr[E_current][rand_ind],:]\n",
    "            labels=Y_tr[tr[E_current][rand_ind],:]\n",
    "\n",
    "            _=sess.run([t for t in train],feed_dict={y_true:labels,s:images})\n",
    "\n",
    "            if n==n_switch[E_current] or n==1:\n",
    "\n",
    "                print(n)\n",
    "\n",
    "                for i in range(N_copies):\n",
    "\n",
    "                    matches=tf.equal(tf.argmax(y_true,1),tf.argmax(y[i],1))\n",
    "                    p=tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "\n",
    "                    pra_test=np.zeros([N_switch])\n",
    "                    cl_test=np.zeros([N_switch])\n",
    "\n",
    "                    pra_val=np.zeros([N_switch])\n",
    "                    cl_val=np.zeros([N_switch])                    \n",
    "\n",
    "                    state_test_spec=[]\n",
    "                    state_val_spec=[]\n",
    "\n",
    "                    for switch in range(N_switch):\n",
    "                    \n",
    "                        # TEST SET:\n",
    "\n",
    "                        images=States_te[te[switch][:],:]\n",
    "                        labels=Y_te[te[switch][:],:]\n",
    "                        \n",
    "                        if MODEL==1:\n",
    "                            \n",
    "                            # TEST SET:\n",
    "                            \n",
    "                            state_,pra_,labels_test=sess.run([state_sparse[i],p,y_true],feed_dict={y_true:labels,s:images})\n",
    "                            cl_=np.sum(state_!=0)/np.sum(N*np.sum(np.sum(X_te[te[switch][:],:,:]!=0,1)!=0,1))\n",
    "\n",
    "                            pra_test[switch]=np.copy(pra_)\n",
    "                            cl_test[switch]=np.copy(cl_)\n",
    "\n",
    "                            state_test_spec.append(state_)\n",
    "\n",
    "                            # VAL SET:\n",
    "\n",
    "                            images=States_val[val[switch][:],:]\n",
    "                            labels=Y_val[val[switch][:],:]\n",
    "\n",
    "                            state_,pra_,labels_val=sess.run([state_sparse[i],p,y_true],feed_dict={y_true:labels,s:images})\n",
    "                            cl_=np.sum(state_!=0)/np.sum(N*np.sum(np.sum(X_val[val[switch][:],:,:]!=0,1)!=0,1))\n",
    "\n",
    "                            pra_val[switch]=np.copy(pra_)\n",
    "                            cl_val[switch]=np.copy(cl_)\n",
    "\n",
    "                            state_val_spec.append(state_)\n",
    "                    \n",
    "                        if MODEL==2:\n",
    "                            \n",
    "                            # TEST SET:\n",
    "                            \n",
    "                            pra_,labels_test=sess.run([p,y_true],feed_dict={y_true:labels,s:images})\n",
    "\n",
    "                            pra_test[switch]=np.copy(pra_)\n",
    "\n",
    "                            # VAL SET:\n",
    "\n",
    "                            images=States_val[val[switch][:],:]\n",
    "                            labels=Y_val[val[switch][:],:]\n",
    "\n",
    "                            pra_,labels_val=sess.run([p,y_true],feed_dict={y_true:labels,s:images})\n",
    "\n",
    "                            pra_val[switch]=np.copy(pra_)\n",
    "                    \n",
    "                    pra_save[i,:,index_help]=np.copy(pra_test)\n",
    "                    \n",
    "                    if MODEL==1:\n",
    "                    \n",
    "                        sp_tasks, sp1_tasks, overlap, overlap1, C_active=specificity(state_test_spec,labels_test,N_te,in_task=False)\n",
    "                        sp_tasks_val, sp1_tasks_val, overlap_val, overlap1_val, C_active_val=specificity(state_val_spec,labels_val,N_val,in_task=False)\n",
    "                        \n",
    "                        cl_save[i,:,index_help]=np.copy(cl_test)\n",
    "                        sp_tasks_save[i,:,index_help]=sp_tasks\n",
    "                        sp1_tasks_save[i,:,index_help]=sp1_tasks\n",
    "                        overlap_save[i,:,:,index_help]=overlap\n",
    "                        overlap1_save[i,:,:,index_help]=overlap1\n",
    "                        C_active_save[i,:,index_help]=C_active\n",
    "                        \n",
    "                        print('Test:', ' Iteration: ',n,'Reservoir ',i,'Probability: ',pra_test ,'Coding ', cl_test)\n",
    "                        #print('Val:', ' Iteration: ',n,'Reservoir ',i,'Probability: ',pra_val ,'Coding ', cl_val)\n",
    "\n",
    "                        print('Overlap', np.mean(overlap), 'Relative ', np.mean(overlap1), 'Active tasks ', np.mean(C_active), 'Spec ' ,np.mean(sp_tasks), ' ', np.mean(sp1_tasks))\n",
    "                        #print('Overlap', np.mean(overlap_val), 'Relative ', np.mean(overlap1_val), 'Active tasks ', np.mean(C_active_val), 'Spec ' ,np.mean(sp_tasks_val), ' ', np.mean(sp1_tasks_val))\n",
    "\n",
    "                    \n",
    "                    if MODEL==2:\n",
    "                        \n",
    "                        print('Test:', ' Iteration: ',n,'Reservoir ',i,'Probability: ',pra_test)\n",
    "                        #print('Val:', ' Iteration: ',n,'Reservoir ',i,'Probability: ',pra_val)\n",
    "\n",
    "\n",
    "\n",
    "                index_help+=1\n",
    "\n",
    "        #io.savemat(\"overlap1_theta0_t15_\"+repr(k)+\".mat\",{\"array\": overlap_save})\n",
    "        #io.savemat(\"overlap2_theta0_t15_\"+repr(k)+\".mat\",{\"array\": overlap1_save})\n",
    "        #io.savemat(\"sp1_theta0_t15_\"+repr(k)+\".mat\",{\"array\": sp_tasks_save})\n",
    "        #io.savemat(\"sp2_theta0_t15_\"+repr(k)+\".mat\",{\"array\": sp1_tasks_save})\n",
    "        #io.savemat(\"C_active_theta0_t15_\"+repr(k)+\".mat\",{\"array\": C_active_save})\n",
    "        #io.savemat(\"Acc_theta0_t15_\"+repr(k)+\".mat\",{\"array\": pra_save})\n",
    "        #io.savemat(\"CL_theta0_t15_\"+repr(k)+\".mat\",{\"array\": cl_save})\n",
    "        #io.savemat(\"N_te_theta0_t15_\"+repr(k)+\".mat\",{\"array\": N_te})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13.1",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
