{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for the computation of the ESN activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Echo:\n",
    "    \n",
    "    def __init__(self,dt,tau_m,tau_M,diluition,N,W_in):\n",
    "            \n",
    "            self.dt=dt\n",
    "            self.N=N\n",
    "            self.tau_m=tau_m\n",
    "            self.tau_M=tau_M\n",
    "            self.Diluition=diluition\n",
    "            \n",
    "            W_np=np.random.uniform(-1,1,[N,N])\n",
    "            D=np.random.uniform(0,1,(N,N))>np.ones((N,N))*diluition\n",
    "            W_np=W_np*D.astype(int)\n",
    "            \n",
    "            eig=np.linalg.eigvals(W_np)\n",
    "            self.eig=eig\n",
    "            \n",
    "            alpha_np=dt/(2*tau_m)\n",
    "            pho_np=1-2*tau_m/tau_M\n",
    "            \n",
    "            W_np=pho_np*W_np/(np.max(np.absolute(eig)))\n",
    "            \n",
    "            self.W_np=W_np\n",
    "            self.alpha_np=alpha_np\n",
    "            self.pho_np=pho_np\n",
    "            \n",
    "            self.W=tf.Variable(W_np,trainable=False,dtype=tf.float32)\n",
    "            self.alpha=tf.constant(alpha_np,dtype=tf.float32)\n",
    "            \n",
    "            self.W_in_np=W_in\n",
    "            self.W_in=tf.Variable(W_in,trainable=False,dtype=tf.float32)\n",
    "            self.N_in=np.shape(W_in)[0]\n",
    "            \n",
    "    def evolution_graph(self,T,init_state,inputs):\n",
    "\n",
    "        \n",
    "        state_hidden=init_state\n",
    "        \n",
    "        states_hidden=[]\n",
    "        xs=[]\n",
    "        \n",
    "        for t in range(T):\n",
    "            \n",
    "            prev_state=tf.identity(state_hidden)\n",
    "            state_hidden=( (1-self.alpha)*prev_state+self.alpha*tf.nn.relu( tf.matmul(prev_state,self.W)+tf.matmul(inputs[:,:,t],self.W_in) )) \n",
    "                        \n",
    "            states_hidden.append(state_hidden)\n",
    "                        \n",
    "        states=tf.concat([tf.expand_dims(s,2) for s in states_hidden],2)\n",
    "\n",
    "    \n",
    "        return state_hidden, states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the input connectivity matrix\n",
    "\n",
    "The weigths of the input connectivity follow a lognormal distribution. The number of connections to an input node is \n",
    "inversely proportional to their value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_input(n_proj,N,n_mean_conn):\n",
    "    \n",
    "    m = 1\n",
    "    v = 0.5\n",
    "    mu = np.log((m**2)/np.sqrt(v+m**2))\n",
    "    sigma = np.sqrt(np.log(v/(m**2)+1))\n",
    "    X= np.random.lognormal(mu, sigma, N)\n",
    "\n",
    "    alpha=np.mean(np.round(X))*n_mean_conn\n",
    "    epsilon=0.3\n",
    "    \n",
    "    while np.mean(np.round(alpha/X))<n_mean_conn-epsilon or np.mean(np.round(alpha/X))>n_mean_conn+epsilon:\n",
    "\n",
    "        if  np.mean(np.round(alpha/X))<n_mean_conn-epsilon:\n",
    "            alpha=alpha+0.01 \n",
    "\n",
    "        if  np.mean(np.round(alpha/X))>n_mean_conn+epsilon:\n",
    "            alpha=alpha-0.01\n",
    "\n",
    "\n",
    "    Y=np.round(alpha/X).astype(int)\n",
    "    W_In=np.zeros([N,N_proj])\n",
    "\n",
    "    for i in range(N):\n",
    "        W_In[i,np.random.randint(0,N_proj,Y[i])]=X[i]\n",
    "\n",
    "    return W_In, X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the input\n",
    "\n",
    "Specifying N_basis and N_class, the following class creates N_class x N_class x N_basis x Sequence_lenght sequences.\n",
    "The procedure will generate sequences and labels that will test the memory capacity of the network.\n",
    "For a description of the procedure, see Appendix of the paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class input_stimuli:\n",
    "    \n",
    "    def __init__(self,T,N_basis,N_class,T_odour):\n",
    "        \n",
    "        import scipy.io\n",
    "        odours = scipy.io.loadmat('hallem_olsen.mat')\n",
    "        odours.keys()\n",
    "        odours=odours['hallem_olsen']\n",
    "        \n",
    "        self.odours=odours/(np.max(np.max(odours,0)))                         # Normalisation of the data\n",
    "        self.N_odours=np.shape(odours)[0]\n",
    "        \n",
    "        \n",
    "        self.T=T\n",
    "        self.N_class=N_class\n",
    "                \n",
    "        Sequence_length=3\n",
    "        N_basis=N_class*N_basis\n",
    "        \n",
    "        \n",
    "        N_sequence=(Sequence_length*N_class)*N_basis\n",
    "        self.N_stimuli=N_sequence\n",
    "        \n",
    "        Sequence_basis=np.zeros([N_basis,Sequence_length])\n",
    "        \n",
    "        vector_index=np.ones([self.N_odours]);\n",
    "        vector_index=vector_index.astype(int)\n",
    "        \n",
    "        # The loop defines, for each output class, a set of sequences (that in the paper we call 'context') composed by elements without repetitions \n",
    "        for i in range(N_class):\n",
    "            \n",
    "            random_int=np.random.permutation(self.N_odours)\n",
    "        \n",
    "            Sequence_basis[i*np.int(N_basis/N_class):(i+1)*np.int(N_basis/N_class),:]=np.reshape(random_int[0:Sequence_length*np.int(N_basis/N_class)],[np.int(N_basis/N_class),Sequence_length])\n",
    "        \n",
    "        \n",
    "        Sequence=np.zeros([N_basis*Sequence_length*N_class,Sequence_length])\n",
    "        Seq_basis=np.zeros([N_basis*Sequence_length*N_class,Sequence_length])\n",
    "        \n",
    "        # It replicates each sequence in the array Sequence_basis N_class*Sequence_length times to apply (later) 'perturbations' \n",
    "        # to its elements (see paper)  \n",
    "        for i in range(np.shape(Sequence_basis)[0]):\n",
    "            \n",
    "            Seq_basis[N_class*Sequence_length*i:N_class*Sequence_length*i+N_class*Sequence_length,:]=Sequence_basis[i,:]\n",
    "        \n",
    "        Sequence=np.copy(Seq_basis)\n",
    "        self.Sequence_basis=Seq_basis  \n",
    "        \n",
    "        ordered=np.linspace(0,self.N_odours-1,self.N_odours)\n",
    "        permutated=np.random.permutation(ordered)\n",
    "    \n",
    "        # The multiples of N_split define when the perturbations are repeated\n",
    "        N_split=int(np.shape(Seq_basis)[0]/N_class)\n",
    "        \n",
    "        # The loop applies 'perturbations' (change of elements) to the sequences \n",
    "        for i in range(int(N_basis/N_class)):\n",
    "                                    \n",
    "            \n",
    "            for j in range(N_class*Sequence_length):\n",
    "                \n",
    "                # Index of the element that is changed\n",
    "                h=np.floor(j/N_class).astype(int)\n",
    "                \n",
    "                for l in range(N_class):\n",
    "                    \n",
    "                    if l==0:\n",
    "                        \n",
    "                        Sequence[N_class*Sequence_length*i+j,h]=permutated[N_class*Sequence_length*i+j]\n",
    "                    \n",
    "                    # Repetitions of the perturbation \n",
    "                    Sequence[N_class*Sequence_length*i+j+N_split*l,h]=permutated[N_class*Sequence_length*i+j]\n",
    "                \n",
    "        Sequence=Sequence.astype(int)\n",
    "        \n",
    "        self.Sequence=Sequence        \n",
    "        \n",
    "        # Sequence is the array of sequences, it is a dictionary for the multi-d response of the input nodes.\n",
    "        \n",
    "        # Each element of each sequence corresponds to a vector in the odours array, containing the response of the input nodes\n",
    "        # to the stimulus. Such vector is then is expanded for the amount of time T_odour that the NN is subjected to it.\n",
    "        stimuli=np.zeros([self.N_stimuli,np.shape(odours)[1],Sequence_length*T_odour])\n",
    "        for i in range(Sequence_length):\n",
    "            \n",
    "            stimuli[0:self.N_stimuli,0:np.shape(odours)[1],T_odour*i:T_odour*i+T_odour]=np.tile(np.expand_dims(self.odours[Sequence[:,i],:],2),[1,1,T_odour])\n",
    "        \n",
    "        \n",
    "        self.stimuli=tf.constant(stimuli,dtype=tf.float32)\n",
    "        \n",
    "        # Definition of the desired output for the classification\n",
    "        label=np.zeros([self.N_stimuli,N_class])\n",
    "        \n",
    "        # Array that helps to build the labels of the dataset of sequence\n",
    "        label_basis=np.zeros([N_class,N_class,N_class])\n",
    "        \n",
    "        # Starting from an identity matrix, it builds the rest of the labels for  by shifting across the columns\n",
    "        label_basis[:,:,0]=np.eye(N_class)\n",
    "        for i in range(N_class-1):\n",
    "            label_basis[:,:,i+1]=np.concatenate((label_basis[:,i+1:,0],label_basis[:,:i+1,0]),1)\n",
    "        \n",
    "        # It repeats the pattern of labels defined above for each set composed by N_split sequences (thus N_split/N_class times) \n",
    "        for i in range(N_class):\n",
    "            label[i*N_split:i*N_split+N_split,:]=np.tile(label_basis[:,:,i],[int(N_split/N_class),1])\n",
    "         \n",
    "        self.labels=tf.constant(label,dtype=tf.float32)\n",
    "        self.labels_np=label\n",
    "        \n",
    "        # Now that the labels are defined, we added multiplicative white noise to the data (the realisation of the noise \n",
    "        # is different for each sample). This is done in 'next_batch'...\n",
    "        \n",
    "    def next_batch(self,rand_ind,N_proj,sigma_noise):\n",
    "                \n",
    "        s_=tf.gather_nd(self.stimuli,rand_ind)\n",
    "        \n",
    "        s=s_+tf.random_normal(tf.shape(s_),0)*s_*sigma_noise\n",
    "        \n",
    "        labels=tf.gather_nd(self.labels,rand_ind)\n",
    "        \n",
    "        return s,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for the computation of the specificity\n",
    "\n",
    "The arguments are:\n",
    "<ul>\n",
    "    <li> state: the values of activities across the set of sequences considered (state has the dimensionality [Number of seq, Number of nodes]) </li>\n",
    "    <li> labels: the desired output of the classification (dimensionality [Number of seq, Number of Classes]) </li>\n",
    "    \n",
    "The function will compute 2 specificity metrics, based on two different normalisations (see paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(state,labels):\n",
    "    \n",
    "    N_class=np.shape(labels)[1]\n",
    "    N=np.shape(state)[1]\n",
    "    N_test=np.shape(labels)[0]\n",
    "    \n",
    "    # It contains the number of times that a node is active for each class \n",
    "    activation_class=np.zeros([N,N_class]) \n",
    "    \n",
    "    # For each separate class, the loop computes the number of times a node is active\n",
    "    for i in range(N_class):\n",
    "        \n",
    "        # Filters the activity with the binary mask np.tile(np.expand_dims(labels[:,i],1),[1,N]) to consider only the sequences \n",
    "        # whose desired class is i \n",
    "        activity_class=state*np.tile(np.expand_dims(labels[:,i],1),[1,N])\n",
    "        \n",
    "        # Summation over the positive values across the first dimension, i.e. different sequences\n",
    "        activation_class[:,i]=np.transpose(np.sum((activity_class>0),0))\n",
    "            \n",
    "    # Tensor describing the specificity\n",
    "    spec_tensor=np.zeros([N,N_class,N_class])\n",
    "    spec_tensor1=np.zeros([N,N_class,N_class])\n",
    "    spec_tensor2=np.zeros([N,N_class,N_class])\n",
    "    \n",
    "    # Specificity\n",
    "    spec=np.zeros([N])\n",
    "    spec1=np.zeros([N])\n",
    "    spec2=np.zeros([N])\n",
    "\n",
    "    # Loop over each pair of classes \n",
    "    for i in range(N_class):\n",
    "        \n",
    "        for j in range(N_class):\n",
    "            \n",
    "            # Index to consider when a node is active at least once for the pair of classes considered\n",
    "            index_help=activation_class[:,i]+activation_class[:,j]>0\n",
    "            \n",
    "            # Specificity, normalised with the total number of times the node was active\n",
    "            spec_tensor[index_help,i,j]=np.absolute(activation_class[index_help,i]-activation_class[index_help,j])/(activation_class[index_help,i]+activation_class[index_help,j])\n",
    "            \n",
    "             # Specificity, normalised with the total number of sequences \n",
    "            spec_tensor1[index_help,i,j]=np.absolute(activation_class[index_help,i]-activation_class[index_help,j])/N_test\n",
    "            \n",
    "\n",
    "    for i in range(N):\n",
    "        \n",
    "        if np.sum(spec_tensor[i,:,:]>0)>0:\n",
    "            \n",
    "            # Average over the positive values in the specificity tensors\n",
    "            spec[i]=np.mean(spec_tensor[i,spec_tensor[i,:,:]>0])\n",
    "            spec1[i]=np.mean(spec_tensor1[i,spec_tensor[i,:,:]>0])\n",
    "\n",
    "    return spec1, spec, spec_tensor, activation_class\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow graph for the training\n",
    "\n",
    "The following class handles the training of three possible models:\n",
    "\n",
    "<ul>\n",
    "    <li> HL: an additional hidden layer on the top of the ESN. If scan=True it is possible to compute the performance as the            number of nodes in the hidden layer or the learning rate change. In the code the arrays N_hidden_values (number of              nodes) and alpha_size_values (learning rate) contain the values that are scanned by the algorithm automatically if              scan=True. Of course, it is possible to change them. </li>\n",
    "    <li> SpaRCe: training of N_copies NNs starting from different initial conditions, i.e. different starting sparsity level.            The algorithm is composed by an initialisation phase (it initialises the starting value of the thresholds, theta_g)            and a training phase (where the thresholds theta_i and the output weights W_out are optimised). The thresholds are              theta=theta_g+theta_i, where the first factor corresponds to the initialisation, and the second factor is optimised            (this split is due to convenience, to have the possibility to check each factor separately)  </li>\n",
    "    <li> ESN: Training of the readout of an ESN. It is possible to compute the performance as the learning rate varies, as in            HL. </li>\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Models:\n",
    "    \n",
    "    def __init__(self,N,N_class,alpha_size,batch_size,N_ep):\n",
    "        \n",
    "        self.N=N\n",
    "        \n",
    "        self.N_class=N_class\n",
    "        \n",
    "        self.alpha_size=alpha_size\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.N_episodes=N_ep\n",
    "        \n",
    "        \n",
    "    def HL(self,state,y_true,scan,N_copies,scan_learning_rate=True):\n",
    "        \n",
    "        R=10*y_true\n",
    "        N_hidden=100\n",
    "        \n",
    "        if scan==False:\n",
    "            \n",
    "            alpha_size_values=[self.alpha_size]\n",
    "            N_hidden_values=[N_hidden]\n",
    "        \n",
    "        if scan==True:\n",
    "            \n",
    "            if scan_learning_rate==True:\n",
    "                \n",
    "                alpha_size_values=np.linspace(0.0002,0.0008,N_copies)\n",
    "                N_hidden_values=np.int32(np.ones([N_copies])*N_hidden)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                alpha_size_values=np.ones([N_copies])*self.alpha_size\n",
    "                N_hidden_values=np.linspace(10,100,N_copies)\n",
    "            \n",
    "            \n",
    "        W_hidden=[]\n",
    "        b_hidden=[]\n",
    "        y1=[]\n",
    "        W_out=[]\n",
    "        y2=[]\n",
    "        error=[]\n",
    "        train=[]\n",
    "\n",
    "        for i in range(N_copies):\n",
    "\n",
    "            W_hidden.append(tf.Variable(np.random.uniform(-1,1,[self.N,N_hidden_values[i]])/(self.N/10),dtype=tf.float32))\n",
    "            b_hidden.append(tf.Variable(np.random.uniform(-1,1,[N_hidden_values[i]])/(self.N),dtype=tf.float32))\n",
    "\n",
    "            y1.append(tf.nn.relu(tf.matmul(state,W_hidden[i])+b_hidden[i]))\n",
    "\n",
    "            W_out.append(tf.Variable(np.random.uniform(-1,1,[N_hidden_values[i],self.N_class])/(self.N/10),dtype=tf.float32))\n",
    "\n",
    "            y2.append( tf.matmul(y1[i],W_out[i]) )\n",
    "\n",
    "            error.append(tf.reduce_mean(tf.square(R-y2[i])))\n",
    "\n",
    "            train.append(tf.train.GradientDescentOptimizer(learning_rate=alpha_size_values[i]).minimize(error[i]))\n",
    "            \n",
    "        return y2, error, train \n",
    "    \n",
    "    \n",
    "    def ESN(self,state,y_true,scan,N_copies):\n",
    "        \n",
    "        R=10*y_true\n",
    "        \n",
    "        if scan==False:\n",
    "    \n",
    "            alpha_size_values=[self.alpha_size]\n",
    "                               \n",
    "        \n",
    "        if scan==True:\n",
    "            \n",
    "            alpha_size_values=np.linspace(0.0002,0.0006,N_copies)\n",
    "            \n",
    "        W_out=[]\n",
    "        y=[]\n",
    "        error=[]\n",
    "        train=[]\n",
    "\n",
    "        for i in range(N_copies):\n",
    "\n",
    "            W_out.append(tf.Variable(np.random.uniform(-1,1,[self.N,self.N_class])/(self.N/10),dtype=tf.float32))\n",
    "\n",
    "            y.append( tf.matmul(state,W_out[i]) )\n",
    "\n",
    "            error.append(tf.reduce_mean(tf.square(R-y[i])))\n",
    "\n",
    "            train.append(tf.train.GradientDescentOptimizer(learning_rate=alpha_size_values[i]).minimize(error[i]))\n",
    "\n",
    "        return y, error, train \n",
    "    \n",
    "    def SpaRCe(self,state,y_true,theta_g_start):\n",
    "        \n",
    "        theta_g=[]\n",
    "        theta_i=[]\n",
    "        W_out=[]\n",
    "        state_sparse=[]\n",
    "        y=[]\n",
    "        error=[]\n",
    "        train1=[]\n",
    "        train2=[]\n",
    "        \n",
    "        R=10*y_true\n",
    "        \n",
    "        theta_istart=np.random.randn(N)/N\n",
    "        \n",
    "        for i in range(N_copies):\n",
    "\n",
    "            theta_g.append(tf.Variable(np.expand_dims(theta_g_start[:,i],0), trainable=False, dtype=tf.float32))\n",
    "\n",
    "            theta_i.append(tf.Variable(theta_istart,dtype=tf.float32))\n",
    "\n",
    "            W_out.append(tf.Variable(np.random.uniform(0,1,[N,N_class])/(N/10),dtype=tf.float32))\n",
    "\n",
    "            state_sparse.append(tf.nn.relu(state-theta_g[i]-theta_i[i]))               \n",
    "\n",
    "            y.append(tf.matmul(state_sparse[i],W_out[i]))\n",
    "\n",
    "            error.append(tf.reduce_mean(tf.square(R-y[i])))\n",
    "\n",
    "            train1.append(tf.train.GradientDescentOptimizer(learning_rate=alpha_size).minimize(error[i],var_list=[W_out[i]]))\n",
    "            train2.append(tf.train.GradientDescentOptimizer(learning_rate=alpha_size/10).minimize(error[i],var_list=[theta_i[i]]))\n",
    "\n",
    "            train=train1+train2\n",
    "        \n",
    "        \n",
    "        return state_sparse, y, error, train\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, main cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[<tf.Operation 'GradientDescent_124' type=NoOp>]\n",
      "1\n",
      "[<tf.Operation 'GradientDescent_124' type=NoOp>, <tf.Operation 'GradientDescent_125' type=NoOp>]\n",
      "2\n",
      "[<tf.Operation 'GradientDescent_124' type=NoOp>, <tf.Operation 'GradientDescent_125' type=NoOp>, <tf.Operation 'GradientDescent_126' type=NoOp>]\n",
      "3\n",
      "[<tf.Operation 'GradientDescent_124' type=NoOp>, <tf.Operation 'GradientDescent_125' type=NoOp>, <tf.Operation 'GradientDescent_126' type=NoOp>, <tf.Operation 'GradientDescent_127' type=NoOp>]\n",
      "4\n",
      "[<tf.Operation 'GradientDescent_124' type=NoOp>, <tf.Operation 'GradientDescent_125' type=NoOp>, <tf.Operation 'GradientDescent_126' type=NoOp>, <tf.Operation 'GradientDescent_127' type=NoOp>, <tf.Operation 'GradientDescent_128' type=NoOp>]\n",
      "5\n",
      "[<tf.Operation 'GradientDescent_124' type=NoOp>, <tf.Operation 'GradientDescent_125' type=NoOp>, <tf.Operation 'GradientDescent_126' type=NoOp>, <tf.Operation 'GradientDescent_127' type=NoOp>, <tf.Operation 'GradientDescent_128' type=NoOp>, <tf.Operation 'GradientDescent_129' type=NoOp>]\n",
      "6\n",
      "[<tf.Operation 'GradientDescent_124' type=NoOp>, <tf.Operation 'GradientDescent_125' type=NoOp>, <tf.Operation 'GradientDescent_126' type=NoOp>, <tf.Operation 'GradientDescent_127' type=NoOp>, <tf.Operation 'GradientDescent_128' type=NoOp>, <tf.Operation 'GradientDescent_129' type=NoOp>, <tf.Operation 'GradientDescent_130' type=NoOp>]\n",
      "7\n",
      "[<tf.Operation 'GradientDescent_124' type=NoOp>, <tf.Operation 'GradientDescent_125' type=NoOp>, <tf.Operation 'GradientDescent_126' type=NoOp>, <tf.Operation 'GradientDescent_127' type=NoOp>, <tf.Operation 'GradientDescent_128' type=NoOp>, <tf.Operation 'GradientDescent_129' type=NoOp>, <tf.Operation 'GradientDescent_130' type=NoOp>, <tf.Operation 'GradientDescent_131' type=NoOp>]\n",
      "8\n",
      "[<tf.Operation 'GradientDescent_124' type=NoOp>, <tf.Operation 'GradientDescent_125' type=NoOp>, <tf.Operation 'GradientDescent_126' type=NoOp>, <tf.Operation 'GradientDescent_127' type=NoOp>, <tf.Operation 'GradientDescent_128' type=NoOp>, <tf.Operation 'GradientDescent_129' type=NoOp>, <tf.Operation 'GradientDescent_130' type=NoOp>, <tf.Operation 'GradientDescent_131' type=NoOp>, <tf.Operation 'GradientDescent_132' type=NoOp>]\n",
      "9\n",
      "[<tf.Operation 'GradientDescent_124' type=NoOp>, <tf.Operation 'GradientDescent_125' type=NoOp>, <tf.Operation 'GradientDescent_126' type=NoOp>, <tf.Operation 'GradientDescent_127' type=NoOp>, <tf.Operation 'GradientDescent_128' type=NoOp>, <tf.Operation 'GradientDescent_129' type=NoOp>, <tf.Operation 'GradientDescent_130' type=NoOp>, <tf.Operation 'GradientDescent_131' type=NoOp>, <tf.Operation 'GradientDescent_132' type=NoOp>, <tf.Operation 'GradientDescent_133' type=NoOp>]\n",
      "Iteration:  5000 ESN_HL  0 Probability:  0.5 Error:  25.061934\n",
      "Iteration:  5000 ESN_HL  1 Probability:  0.5 Error:  25.05164\n",
      "Iteration:  5000 ESN_HL  2 Probability:  0.5 Error:  25.049086\n",
      "Iteration:  5000 ESN_HL  3 Probability:  0.5 Error:  25.046297\n",
      "Iteration:  5000 ESN_HL  4 Probability:  0.5 Error:  24.9949\n",
      "Iteration:  5000 ESN_HL  5 Probability:  0.5 Error:  24.993431\n",
      "Iteration:  5000 ESN_HL  6 Probability:  0.5 Error:  24.978882\n",
      "Iteration:  5000 ESN_HL  7 Probability:  0.5 Error:  25.01472\n",
      "Iteration:  5000 ESN_HL  8 Probability:  0.5203125 Error:  24.960735\n",
      "Iteration:  5000 ESN_HL  9 Probability:  0.50416666 Error:  24.716383\n",
      "Iteration:  10000 ESN_HL  0 Probability:  0.55677086 Error:  24.991547\n",
      "Iteration:  10000 ESN_HL  1 Probability:  0.58802086 Error:  24.71754\n",
      "Iteration:  10000 ESN_HL  2 Probability:  0.6109375 Error:  24.523668\n",
      "Iteration:  10000 ESN_HL  3 Probability:  0.6692708 Error:  23.407877\n",
      "Iteration:  10000 ESN_HL  4 Probability:  0.6328125 Error:  22.672962\n",
      "Iteration:  10000 ESN_HL  5 Probability:  0.64270836 Error:  22.074959\n",
      "Iteration:  10000 ESN_HL  6 Probability:  0.6927083 Error:  20.574818\n",
      "Iteration:  10000 ESN_HL  7 Probability:  0.6953125 Error:  20.964376\n",
      "Iteration:  10000 ESN_HL  8 Probability:  0.703125 Error:  20.469973\n",
      "Iteration:  10000 ESN_HL  9 Probability:  0.72447914 Error:  19.934704\n",
      "Iteration:  15000 ESN_HL  0 Probability:  0.5 Error:  24.74186\n",
      "Iteration:  15000 ESN_HL  1 Probability:  0.6234375 Error:  22.529812\n",
      "Iteration:  15000 ESN_HL  2 Probability:  0.6453125 Error:  20.930042\n",
      "Iteration:  15000 ESN_HL  3 Probability:  0.66510415 Error:  20.596514\n",
      "Iteration:  15000 ESN_HL  4 Probability:  0.61354166 Error:  22.1992\n",
      "Iteration:  15000 ESN_HL  5 Probability:  0.728125 Error:  17.835842\n",
      "Iteration:  15000 ESN_HL  6 Probability:  0.7640625 Error:  15.843555\n",
      "Iteration:  15000 ESN_HL  7 Probability:  0.6984375 Error:  19.559977\n",
      "Iteration:  15000 ESN_HL  8 Probability:  0.6635417 Error:  20.057194\n",
      "Iteration:  15000 ESN_HL  9 Probability:  0.62708336 Error:  22.91224\n",
      "Iteration:  20000 ESN_HL  0 Probability:  0.6635417 Error:  22.690136\n",
      "Iteration:  20000 ESN_HL  1 Probability:  0.734375 Error:  18.147762\n",
      "Iteration:  20000 ESN_HL  2 Probability:  0.73854166 Error:  16.765232\n",
      "Iteration:  20000 ESN_HL  3 Probability:  0.79114586 Error:  14.980681\n",
      "Iteration:  20000 ESN_HL  4 Probability:  0.7317708 Error:  17.224508\n",
      "Iteration:  20000 ESN_HL  5 Probability:  0.76979166 Error:  15.474667\n",
      "Iteration:  20000 ESN_HL  6 Probability:  0.81458336 Error:  13.975808\n",
      "Iteration:  20000 ESN_HL  7 Probability:  0.86302084 Error:  12.660796\n",
      "Iteration:  20000 ESN_HL  8 Probability:  0.80052084 Error:  13.681983\n",
      "Iteration:  20000 ESN_HL  9 Probability:  0.74791664 Error:  16.618036\n",
      "Iteration:  25000 ESN_HL  0 Probability:  0.7473958 Error:  18.372171\n",
      "Iteration:  25000 ESN_HL  1 Probability:  0.79114586 Error:  14.506215\n",
      "Iteration:  25000 ESN_HL  2 Probability:  0.92395836 Error:  8.513015\n",
      "Iteration:  25000 ESN_HL  3 Probability:  0.75989586 Error:  16.307798\n",
      "Iteration:  25000 ESN_HL  4 Probability:  0.840625 Error:  14.026761\n",
      "Iteration:  25000 ESN_HL  5 Probability:  0.79270834 Error:  13.95493\n",
      "Iteration:  25000 ESN_HL  6 Probability:  0.8296875 Error:  12.765112\n",
      "Iteration:  25000 ESN_HL  7 Probability:  0.79895836 Error:  13.874494\n",
      "Iteration:  25000 ESN_HL  8 Probability:  0.74114585 Error:  17.674263\n",
      "Iteration:  25000 ESN_HL  9 Probability:  0.7661458 Error:  15.570017\n",
      "Iteration:  30000 ESN_HL  0 Probability:  0.7765625 Error:  15.451021\n",
      "Iteration:  30000 ESN_HL  1 Probability:  0.9036458 Error:  9.878306\n",
      "Iteration:  30000 ESN_HL  2 Probability:  0.928125 Error:  8.519024\n",
      "Iteration:  30000 ESN_HL  3 Probability:  0.9140625 Error:  9.394246\n",
      "Iteration:  30000 ESN_HL  4 Probability:  0.9265625 Error:  8.845972\n",
      "Iteration:  30000 ESN_HL  5 Probability:  0.753125 Error:  17.001854\n",
      "Iteration:  30000 ESN_HL  6 Probability:  0.57395834 Error:  27.472258\n",
      "Iteration:  30000 ESN_HL  7 Probability:  0.84739584 Error:  12.904965\n",
      "Iteration:  30000 ESN_HL  8 Probability:  0.8489583 Error:  11.992266\n",
      "Iteration:  30000 ESN_HL  9 Probability:  0.871875 Error:  12.142645\n",
      "Iteration:  35000 ESN_HL  0 Probability:  0.7734375 Error:  15.162156\n",
      "Iteration:  35000 ESN_HL  1 Probability:  0.9369792 Error:  7.3478837\n",
      "Iteration:  35000 ESN_HL  2 Probability:  0.9848958 Error:  3.9394364\n",
      "Iteration:  35000 ESN_HL  3 Probability:  0.9223958 Error:  8.253432\n",
      "Iteration:  35000 ESN_HL  4 Probability:  0.93333334 Error:  7.7210774\n",
      "Iteration:  35000 ESN_HL  5 Probability:  0.9380208 Error:  6.5587597\n",
      "Iteration:  35000 ESN_HL  6 Probability:  0.8515625 Error:  11.04491\n",
      "Iteration:  35000 ESN_HL  7 Probability:  0.92291665 Error:  8.624048\n",
      "Iteration:  35000 ESN_HL  8 Probability:  0.75989586 Error:  15.61514\n",
      "Iteration:  35000 ESN_HL  9 Probability:  0.89635414 Error:  10.6978035\n",
      "Iteration:  40000 ESN_HL  0 Probability:  0.759375 Error:  16.224691\n",
      "Iteration:  40000 ESN_HL  1 Probability:  0.7864583 Error:  14.807529\n",
      "Iteration:  40000 ESN_HL  2 Probability:  0.9848958 Error:  4.2346673\n",
      "Iteration:  40000 ESN_HL  3 Probability:  0.92395836 Error:  7.466743\n",
      "Iteration:  40000 ESN_HL  4 Probability:  0.97760415 Error:  4.2453203\n",
      "Iteration:  40000 ESN_HL  5 Probability:  0.9734375 Error:  5.541514\n",
      "Iteration:  40000 ESN_HL  6 Probability:  0.8442708 Error:  12.041838\n",
      "Iteration:  40000 ESN_HL  7 Probability:  0.8671875 Error:  10.829599\n",
      "Iteration:  40000 ESN_HL  8 Probability:  0.8510417 Error:  12.01677\n",
      "Iteration:  40000 ESN_HL  9 Probability:  0.8197917 Error:  13.429291\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-99a9a0583661>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mind_next\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mN_stimuli\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minit_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mind_nextbatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mind_next\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_episodes\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mN_check\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "T=30                        # Time steps in a sequence\n",
    "N_class=2                   # Number of output classes\n",
    "N=1000                      # Number of nodes in the ESN\n",
    "T_odour=10                  # Temporal lenght of an element in the sequence\n",
    "\n",
    "dt=0.01                     # time frame\n",
    "tau_m=0.05                  # Minimum timescale in the ESN\n",
    "tau_M=2                     # Maximum timescale in the ESN (the timescales of the ESN will be approximatly inside the interval\n",
    "                            # [tau_m tau_M] )\n",
    "    \n",
    "diluition=0.99              # Probability of a zero in the connectivity matrix of the ESN\n",
    "N_copies=10                 # Number of ESNs to be trained, starting from different initial conditions of the thresholds (theta_g)\n",
    "\n",
    "N_episodes=100000           # Number of training instances\n",
    "N_check=20                  # Number of times the code computes the performance, equally spaced across the simulation time\n",
    "    \n",
    "rescale_W_in=1              # Multiplicative factor of the input connectivity\n",
    "\n",
    "N_basis=16\n",
    "sigma_noise=0.2\n",
    "\n",
    "inputs=input_stimuli(T,N_basis,N_class,T_odour)\n",
    "            \n",
    "N_proj=np.shape(inputs.odours)[1]\n",
    "\n",
    "# Indexes to test the performance of the network\n",
    "index_test_=np.arange(0,inputs.N_stimuli,dtype=int)\n",
    "N_epochs=10\n",
    "index_test_=np.tile(index_test_,[N_epochs])\n",
    "index_test=np.zeros([np.shape(index_test_)[0],1])\n",
    "index_test[:,0]=index_test_\n",
    "\n",
    "## RNN DEFINITION AND TENSORFLOW GRAPH FOR THE ACTIVITY OF THE ESN AND SPARCE\n",
    "\n",
    "## RNN DEFINITION\n",
    "\n",
    "W_in,X=W_input(N_proj,N,n_mean_conn=6)\n",
    "W_in=W_in.T/rescale_W_in\n",
    "\n",
    "rnn=Echo(dt,tau_m,tau_M,diluition,N,W_in)\n",
    "\n",
    "init_state=tf.placeholder(tf.float32,[None,N])\n",
    "ind_nextbatch=tf.placeholder(tf.int32,[None,1])\n",
    "\n",
    "stimuli,y_true=inputs.next_batch(ind_nextbatch,N_proj,sigma_noise)\n",
    "\n",
    "state,states=rnn.evolution_graph(T,init_state,stimuli)\n",
    "\n",
    "\n",
    "## MODEL DEFINITION\n",
    "# MODEL=1 -> SpaRCe\n",
    "# MODEL=2 -> HL, additional NN layer after the ESN\n",
    "# MODEL=3 -> Simple ESN\n",
    "# The learning rates and the batch_sizes specified for the different models are good possible values, selected after grid search\n",
    "\n",
    "MODEL=2\n",
    "\n",
    "N_copies=10\n",
    "\n",
    "if MODEL==1:\n",
    "    \n",
    "    alpha_size=0.002                                                  # Learning rate\n",
    "    batch_size=30    \n",
    "    NN_model=Models(N,N_class,alpha_size,batch_size,N_episodes)\n",
    "    \n",
    "    theta_g_start=np.zeros([N,N_copies])\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        state_=sess.run(state,feed_dict={init_state:np.zeros([np.shape(index_test)[0],N]),ind_nextbatch:index_test})\n",
    "\n",
    "        for i in range(N_copies):\n",
    "\n",
    "            theta_g=np.percentile(state_,i*N_copies,0)\n",
    "\n",
    "            x=(state_-theta_g)>0\n",
    "            theta_g_start[:,i]=theta_g\n",
    "    \n",
    "    state_sparse, y, error, train = NN_model.SpaRCe(state,y_true,theta_g_start)\n",
    "    \n",
    "if MODEL==2:\n",
    "    \n",
    "    alpha_size=0.0006                                                # Learning rate\n",
    "    batch_size=30   \n",
    "    NN_model=Models(N,N_class,alpha_size,batch_size,N_episodes)      \n",
    "    \n",
    "    scan=True                                                     \n",
    "    \n",
    "    if scan==False:\n",
    "        N_copies=1\n",
    "    \n",
    "    y, error, train = NN_model.HL(state,y_true,scan,N_copies,scan_learning_rate=True)\n",
    "    \n",
    "\n",
    "if MODEL==3:\n",
    "    \n",
    "    alpha_size=0.0005                                               # Learning rate\n",
    "    batch_size=100\n",
    "    \n",
    "    scan=True\n",
    "    \n",
    "    if scan==False:\n",
    "        N_copies=1\n",
    "    \n",
    "    NN_model=Models(N,N_class,alpha_size,batch_size,N_episodes)\n",
    "    y, error, train = NN_model.ESN(state,y_true,scan,N_copies)\n",
    "    \n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for n in range(N_episodes+1):\n",
    "\n",
    "        ind_next=np.random.randint(0,inputs.N_stimuli,[batch_size,1])\n",
    "\n",
    "        _=sess.run([t for t in train],feed_dict={init_state:np.zeros([batch_size,N]),ind_nextbatch:ind_next})\n",
    "\n",
    "        if n%(np.round(N_episodes/N_check))==0 and n>0:\n",
    "\n",
    "            index_help=(n/(np.round(N_episodes/N_check))).astype(int)\n",
    "            \n",
    "            for i in range(N_copies):\n",
    "\n",
    "                matches=tf.equal(tf.argmax(y_true,1),tf.argmax(y[i],1))\n",
    "                p=tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "                \n",
    "                                \n",
    "                if MODEL==1:\n",
    "                    \n",
    "                    labels_,state_,p_ra,error_=sess.run([y_true,state_sparse[i],p,error[i]],feed_dict={init_state:np.zeros([np.shape(index_test)[0],N]),ind_nextbatch:index_test})\n",
    "                    \n",
    "                    coding_level=np.mean(state_>0)\n",
    "                    spec1, spec, spec_tensor, activation_class=specificity(state_,labels_)\n",
    "                    print('Iteration: ',n,'SpaRCe ',i,'Probability: ',p_ra,'Error: ',error_,'Coding: ',coding_level,'Spec: ',np.mean(spec),' ',np.mean(spec1))\n",
    "\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    p_ra,error_=sess.run([p,error[i]],feed_dict={init_state:np.zeros([np.shape(index_test)[0],N]),ind_nextbatch:index_test})\n",
    "\n",
    "                    if MODEL==2:\n",
    "                        print('Iteration: ',n,'ESN_HL ',i,'Probability: ',p_ra,'Error: ',error_)\n",
    "                    if MODEL==3:\n",
    "                        print('Iteration: ',n,'ESN ',i,'Probability: ',p_ra,'Error: ',error_)\n",
    "                        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13.1",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
